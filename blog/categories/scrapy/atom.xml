<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scrapy | Amyangfei's Blog]]></title>
  <link href="http://amyangfei.me/blog/categories/scrapy/atom.xml" rel="self"/>
  <link href="http://amyangfei.me/"/>
  <updated>2015-01-25T20:26:28+08:00</updated>
  <id>http://amyangfei.me/</id>
  <author>
    <name><![CDATA[amyangfei]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[scrapy使用笔记]]></title>
    <link href="http://amyangfei.me/2013/03/16/tips-of-scrapy/"/>
    <updated>2013-03-16T00:00:00+08:00</updated>
    <id>http://amyangfei.me/2013/03/16/tips-of-scrapy</id>
    <content type="html"><![CDATA[<p>最近在使用 scrapy 做爬虫，整个爬取框架很清晰，使用 <a href="http://twistedmatrix.com/trac/" target="_blank">twisted</a> 这个异步网络库来处理网络通讯，由 scrapy engine 作为系统调度。具体使用时通过继承 BaseSpider 自定义爬虫类，用户自己实现具体的页面分析、提供带抓取url，系统会自动调度完成页面的请求、下载返回resonse供解析。对于需要保存的数据，可以送到 Item Pipeline 中做后续处理。</p>

<h3>递归爬取页面</h3>

<p>爬虫运行时不断发现新的页面并进行爬取。跟踪 scrapy 源码中 BaseSpider 类，会发现下边的方法。爬去框架初始运行时会由系统调度，通过调用 next()，在 yield 挂起处继续执行 make_requests_from_url 返回 Request 对象，然后由系统调度进一步请求页面。每次爬去页面返回获得response后执行 spider 中的 parse 函数。</p>

<p>``` python
def start_requests(self):</p>

<pre><code>for url in self.start_urls:
    yield self.make_requests_from_url(url)
</code></pre>

<p>def make_requests_from_url(self, url):</p>

<pre><code>return Request(url, dont_filter=True)
</code></pre>

<h1>code in engine.py/class ExecutionEngine</h1>

<p>try:</p>

<pre><code>request = slot.start_requests.next()
</code></pre>

<p>except StopIteration:</p>

<pre><code>slot.start_requests = None
</code></pre>

<p>except Exception, exc:</p>

<pre><code>log.err(None, 'Obtaining request from start requests', spider=spider)
</code></pre>

<p>else:</p>

<pre><code>self.crawl(request, spider)
</code></pre>

<p>```</p>

<p>爬虫爬取时会不断发现新的 url 要爬取，在scrapy中实现递归爬取的方法很简单，只需要将下面的代码加到 parse 函数中。这样 parse 会在 yield 处挂起，等待系统的调度。下一次调用 next()后返回挂起点继续执行。</p>

<!-- more -->


<p>``` python
yield self.make_requests_from_url(url)</p>

<h1>you can also use the following line</h1>

<p>items.append(self.make_requests_from_url(url))
```</p>

<p>需要注意的是递归调用 make_requests_from_url 的代码 <b>一定写在 parse 函数中</b>，否则如果像下边代码的结构，parse 调用 _handle_page ，当_handle_page 进入 yield 语句块时会挂起，等待 next 信号，但是 parse 函数会运行退出，于是 yield 后的 make_requests_from_url 将不再会被调用，爬取也无法递归的进行下去。我一开始写就遇到了这个问题，分析系统调度过程中使用 yield 之后发现了问题所在。</p>

<p>``` python
def parse(self, response):</p>

<pre><code>page_type = self._gen_type_fromurl(response.url)       
self._handle_page(response, page_type)
</code></pre>

<p>def _handle_page(self, response, page_type):</p>

<pre><code>if page_type == 'subject':
    hxs = HtmlXPathSelector(response)
    new_url = hxs.select('//div[@class="rr"]/a/@href')[0].extract()
    yield self.make_requests_from_url(new_url)
elif page_type == 'review':
    hxs = HtmlXPathSelector(response)
    content = hxs.select('//div[@id="link-report"]/div').select('text()').extract()
</code></pre>

<p>```</p>

<p>下边这个 demo 是上边错误的简化版本，调用 funcb 函数，funcc 函数挂起，funcb 内部的代码执行后 funcb 函数退出。</p>

<p>``` python
def funcb():</p>

<pre><code>print 'fun-b starts'
funcc()
print 'fun-b ends'
</code></pre>

<p>def funcc():</p>

<pre><code>print 'fun-c starts'
yield 'fun-c yield'
</code></pre>

<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:</p>

<pre><code>b = funcb()
</code></pre>

<h1>output</h1>

<p>fun-b starts
fun-b ends  <br/>
```</p>

<h3>Scrapy相关资源链接</h3>

<ol>
<li><a href="http://blog.pluskid.org/?p=366" target="_blank">Scrapy 轻松定制网络爬虫</a></li>
<li><a href="http://www.yakergong.net/blog/archives/500" target="_blank">使用scrapy进行大规模抓取</a></li>
<li><a href="http://isbullsh.it/2012/04/Web-crawling-with-scrapy/" target="_blank">Crawl a website with scrapy</a></li>
</ol>

]]></content>
  </entry>
  
</feed>
