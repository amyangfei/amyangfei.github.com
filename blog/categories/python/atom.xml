<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: python | Amyangfei's Blog]]></title>
  <link href="https://amyangfei.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="https://amyangfei.github.io/"/>
  <updated>2018-08-24T10:52:28+08:00</updated>
  <id>https://amyangfei.github.io/</id>
  <author>
    <name><![CDATA[amyangfei]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[python 拾遗2]]></title>
    <link href="https://amyangfei.github.io/2014/12/04/python-tips-2/"/>
    <updated>2014-12-04T00:00:00+08:00</updated>
    <id>https://amyangfei.github.io/2014/12/04/python-tips-2</id>
    <content type="html"><![CDATA[<p>本文是上一篇文章 <a href="https://amyangfei.github.io/2014/11/27/python-tips/" target="_blank">python 拾遗</a> 的延续，继续整理 python 的一些使用技巧，以及一些可能被忽略的细节</p>

<p>注意: 以下讨论主要为 Python2.7 版本， Python 3 的内容有待跟进</p>

<!-- more -->


<h3>Get MD5 hash of big files</h3>

<hr />

<p>当我们需要通过 python 得到一个很大文件的 md5 值的时候，我们可以通过分段读取文件的方法来节约内存，选择合适的分段大小还会适当提高计算效率。
<a href="https://gist.github.com/amyangfei/37b7d52003f38f8a3877" target="_blank">chksum.py</a> 通过 <code>memory_profiler</code> 统计执行过程中内存的使用情况并统计每一次计算的执行时间，同时给出了1Gb 数据的测试结果。</p>

<p>stackoverflow 上的一些讨论：<a href="http://stackoverflow.com/questions/1131220/get-md5-hash-of-big-files-in-python" target="_blank">Get MD5 hash of big files in Python</a>, <a href="http://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python" target="_blank">Lazy Method for Reading Big File in Python</a></p>

<h3>io.BytesIO vs cString.StringIO</h3>

<hr />

<p>python2 和 python3 在 StringIO 和 BytesIO 之间有诸多不同，<a href="https://pypi.python.org/pypi/six" target="_blank">six</a> 是一个提供同时兼容 py2 和 py3 的解决方案，这个几个模块的具体区别参考下边的表格。</p>

<table>
<thead>
<tr>
<th align="center">模块 </th>
<th align="center"> Python 2 </th>
<th align="center"> Python 3</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">StringIO.StringIO </td>
<td align="center"> 内存中的字符串缓存，可以存储字符串或Unicode 类型 </td>
<td align="center"> 删除</td>
</tr>
<tr>
<td align="center">cStringIO.StringIO </td>
<td align="center"> 基于C实现提供类似StringIO.StringIO的接口且更高效，但是相比StringIO.StringIO使用有一定限制 </td>
<td align="center"> 删除</td>
</tr>
<tr>
<td align="center">io.StringIO </td>
<td align="center">对 Unicode文本内容的内存缓存，只能存储 Unicode 对象 </td>
<td align="center"> 对文本数据的内存缓存，不能接收 Unicode 类型</td>
</tr>
<tr>
<td align="center">io.BytesIO </td>
<td align="center"> 存储字节的内存缓存 </td>
<td align="center"> 存储字节的内存缓存</td>
</tr>
<tr>
<td align="center">six.StringIO </td>
<td align="center"> StringIO.StringIO </td>
<td align="center"> io.StringIO</td>
</tr>
<tr>
<td align="center">six.BytesIO </td>
<td align="center"> StringIO.StringIO </td>
<td align="center"> io.BytesIO</td>
</tr>
</tbody>
</table>


<p>在性能上：通常 cStringIO.StringIO 是最快的。io.Bytes 同样是通过 C 实现的，但是例如通过 <code>io.BytesIO(b'data')</code> 初始化 BytesIO 对象时会对数据进行一次复制，这会引起性能上的损失。</p>

<p>关于 StringIO 和 BytesIO 的性能区别，对于 IO 性能敏感的场景还是有很大影响，例如在 <a href="https://github.com/tornadoweb/tornado/issues/1110" target="_blank">tornado</a>，<a href="https://github.com/scrapy/scrapy/pull/803" target="_blank">scrapy</a> 的项目中以及 <a href="https://mail.python.org/pipermail//python-dev/2014-July/135542.html" target="_blank">Python 邮件列表</a> 中都有相关讨论。
在未来 Python3.5 版本中将会对 io.BytesIO 进行 copy-on-write 的优化，详见：<a href="http://bugs.python.org/issue22003" target="_blank">Python Issue22003</a>。</p>

<p>当具体需要创建 file-like 的数据流时并且需要考虑对 Python2 和 Python3 代码的兼容性时，我们需要根据具体的数据类型（字符串或者 Unicode 或者 Bytes），以及使用场景对性能的要求选择合适的模块。</p>

<h3>List comprehensions leak the loop control variable</h3>

<hr />

<p>看一段很简单的列表生成的代码：</p>

<p>```python</p>

<blockquote><blockquote><blockquote><p>x = &lsquo;before&rsquo;
a = [x for x in range(5)]
x
4
x = &lsquo;before&rsquo;
a = (x for x in range(5))
x
&lsquo;before&rsquo;
```</p></blockquote></blockquote></blockquote>

<p>在 python2.x 中，list comprehension 中变量的作用域并不仅限于 [] 中，而是会泄露出来，而 Generator expressions 执行时会创建一个独立的运行域，因而不会发生变量泄露。在 Python3 中 list comprehension 变量泄露已经得到了修改。</p>

<p>下边是 <a href="http://python-history.blogspot.com/2010/06/from-list-comprehensions-to-generator.html" target="_blank">Python History</a> 中的原文</p>

<blockquote><p>This was an artifact of the original implementation of list comprehensions; it was one of Python&rsquo;s &ldquo;dirty little secrets&rdquo; for years. It started out as an intentional compromise to make list comprehensions blindingly fast, and while it was not a common pitfall for beginners, it definitely stung people occasionally. For generator expressions we could not do this. Generator expressions are implemented using generators, whose execution requires a separate execution frame. Thus, generator expressions (especially if they iterate over a short sequence) were less efficient than list comprehensions.</p></blockquote>

<h3>socket.settimeout(value)</h3>

<hr />

<p>socket 设置超时之后，该 socket 就是 non-blocking 模式</p>

<blockquote><p><strong>Timeout mode internally sets the socket in non-blocking mode.</strong> The blocking and timeout modes are shared between file descriptors and socket objects that refer to the same network endpoint. A consequence of this is that file objects returned by the makefile() method must only be used when the socket is in blocking mode; in timeout or non-blocking mode file operations that cannot be completed immediately will fail.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python 拾遗]]></title>
    <link href="https://amyangfei.github.io/2014/11/27/python-tips/"/>
    <updated>2014-11-27T00:00:00+08:00</updated>
    <id>https://amyangfei.github.io/2014/11/27/python-tips</id>
    <content type="html"><![CDATA[<p>整理 python 使用的一些技巧，以及一些可能被忽略的细节，很多在文档可以查找到的内容将不会过多的描述，更多以外链的形式存在。</p>

<p>注意: 以下讨论主要为 Python2.7 版本， Python 3 的内容有待跟进</p>

<!-- more -->


<h3>import</h3>

<p>python 的 import 通过调用 <code>__import__(name[, globals[, locals[, fromlist[, level]]]])</code> 这个函数实现，借助这个函数可以通过 python 模块的名字动态引用模块。来自tornado 的 <a href="http://tornado.readthedocs.org/en/latest/_modules/tornado/util.html#import_object" target="_blank">import_object</a> 是一个很简洁的封装。</p>

<p>```python
def import_object(name):</p>

<pre><code>"""Imports an object by name.
import_object('x') is equivalent to 'import x'.
import_object('x.y.z') is equivalent to 'from x.y import z'.
"""
if name.count('.') == 0:
    return __import__(name, None, None)

parts = name.split('.')
obj = __import__('.'.join(parts[:-1]), None, None, [parts[-1]], 0)
try:
    return getattr(obj, parts[-1])
except AttributeError:
    raise ImportError("No module named %s" % parts[-1])
</code></pre>

<p>```</p>

<p>除此之外，我们还可以利用 python2.7 开始提供的 <a href="https://docs.python.org/2/library/importlib.html" target="_blank">importlib.import_module</a> （<a href="http://svn.python.org/projects/python/trunk/Lib/importlib/__init__.py" target="_blank">实现代码</a>）来进行动态引用。例如：<code>importlib.import_module(&lsquo;tornado.httpclient&rsquo;)</code></p>

<h3>reload</h3>

<p>当 reload 一个 python 模块之后有两处不会使用新的模块的值：</p>

<ol>
<li><p>原来已经使用的实例还是会使用旧的模块，而新生产的实例会使用新的模块；</p></li>
<li><p>其他模块引用该模块的对象，这些引用不会绑定到新的对象上。</p></li>
</ol>


<p><a href="https://github.com/jparise/python-reloader" target="_blank">python-reloader</a> 是一个很有趣的项目，它将 <code>__builtin__.__import__</code> 修改为自定义的 <code>_import</code> 函数，新的 <code>_import</code> 在原有调用 <code>__import__(name[, globals[, locals[, fromlist[, level]]]])</code> 的同时记录下引用模块之间的依赖关系。python-reloader 实现的是 reload 一个模块之后，reload 该模块所依赖的所有模块，而不是 reload 所有依赖该模块的模块。说起来很绕，这里 <a href="https://github.com/jparise/python-reloader/issues/14" target="_blank">issue(It should reload dependants instead of dependencies)</a>有很好的讨论。在实际运行的系统中动态 reload 模块可能并非一种很好的选择。</p>

<h3>Catching an exception while using &lsquo;with&rsquo;</h3>

<p>当我们使用 with 表达式并且需要捕捉异常的时候，我们可以这样做：</p>

<p>```python
try:</p>

<pre><code>with open( "foo.txt" ) as f :
    print f.readlines()
</code></pre>

<p>except EnvironmentError: # parent of IOError, OSError</p>

<pre><code>print 'oops'
</code></pre>

<p>```</p>

<p>如果希望捕捉 with 表达式的异常与内部工作代码的异常分离出来，我们可以这样做：</p>

<p>```python
try:</p>

<pre><code>f = open('foo.txt')
</code></pre>

<p>except IOError:</p>

<pre><code>print 'oops'
</code></pre>

<p>else:</p>

<pre><code>with f:
    print f.readlines()
</code></pre>

<p>```</p>

<p>关于 python 使用 try-except-else, with 的一些讨论：<a href="http://stackoverflow.com/questions/16138232/is-it-a-good-practice-to-use-try-except-else-in-python" target="_blank">Is it a good practice to use try-except-else</a>，<a href="http://stackoverflow.com/questions/3642080/using-python-with-statement-with-try-except-block" target="_blank">Using python “with” statement with try-except block</a></p>

<h3>StringIO</h3>

<p>当我们使用一些接收参数是文件类型的 API 时，我们可能需要使用到 StringIO，例如使用 gzip 模块压缩一个字符串：</p>

<p>```python
import gzip, StringIO</p>

<p>stringio = StringIO.StringIO()
gzip_file = gzip.GzipFile(fileobj=stringio, mode=&lsquo;w&rsquo;)
gzip_file.write(&lsquo;Hello World&rsquo;)
gzip_file.close()</p>

<p>stringio.getvalue()
```</p>

<p>在 Python2.7 中 <a href="https://docs.python.org/2/library/stringio.html#module-cStringIO" target="_blank">cStringIO</a> 提供了与 StringIO 类似的接口，并且运行效率更高。在 Python3.4 中这两者被统一成为了 io.StringIO。</p>

<p>注意：cStringIO 的使用有一些限制：cStringIO 不能作为基类被继承；cStringIO 不能接收非 ASCII 字符的字符串参数；还有一点与 StringIO 不同的是当使用字符串参数初始化一个 cStringIO 对象时，该对象是只读的。</p>

<h3>Queue.Queue vs collections.deque</h3>

<p>Queue (python3 重命名为 queue)是一个可用于多线程之间同步、交换数据的队列模块，包括 FIFO，LIFO，优先级队列三个实现。</p>

<p>collections.deque 是一个双端队列的数据结构，在头和尾的插入、删除、读取操作是O(1)复杂度；在队列中部的随机读取操作是O(n)的。</p>

<p>reference：<a href="http://stackoverflow.com/questions/717148/python-queue-queue-vs-collections-deque" target="_blank">Python: Queue.Queue vs. collections.deque</a></p>

<h3>heapq</h3>

<p>python 内置的 heapq 是一个小顶堆，并且 <code>heapify</code>, <code>heappush</code>, <code>heappop</code> 操作是不支持传递 <code>key</code> 参数的。如果想实现大顶堆，可以这样 <code>lambda x: -x</code>，或者自己封装一层。例如：
<a href="http://stackoverflow.com/a/14189741/1115857" target="_blank">python topN max heap</a>。另外，<code>heapq.nlargest</code> 和 <code>heapq.nsmallest</code> 支持 key 参数。
邮件列表里的讨论：<a href="http://code.activestate.com/lists/python-list/162387/" target="_blank">为什么 python 的 heapq 没有支持 key 参数</a></p>

<h3>itertools.tee</h3>

<p><code>itertools.tee</code> 从一个迭代器返回 n 个独立的迭代器，原始迭代器将不允许被使用，如果使用，那么可能会导致新的迭代器失效。<a href="http://discontinuously.com/2012/06/inside-python-tee/" target="_blank">Inside Python&rsquo;s itertools.tee</a> 很详细的探究了 <code>itertools.tee</code> 的实现细节。</p>

<h3>when should we use operator</h3>

<p>最常用的就是 <code> operator.itemgetter</code>，例如我们有一个 tuple 列表，需要对这些元组按照第i个元素排序，那么可以这样：<code>lst.sort(key=operator.itemgetter(i))</code>。</p>

<p><code>operator.add</code> 与 <code>lambda x, y: x+y</code> 具有相同的效果，二者的不同主要有两个方面：一方面是它们的可读性、开发者的使用习惯的差别；另一方面是性能差别。在<a href="https://wiki.python.org/moin/PythonSpeed" target="_blank">python wiki</a> 中有这样一段话：</p>

<blockquote><p>Likewise, the builtin functions run faster than hand-built equivalents. For example, map(operator.add, v1, v2) is faster than map(lambda x,y: x+y, v1, v2).</p></blockquote>

<p>我们对二者做一次简单的实验对比：</p>

<p>```python
➜ python -m timeit &lsquo;import operator&rsquo; &lsquo;map(operator.add, [x for x in range(5000)], [y for y in range(5000)])&rsquo;
1000 loops, best of 3: 710 usec per loop</p>

<p>➜ python3 -m timeit &lsquo;import operator&rsquo; &lsquo;map(operator.add, [x for x in range(5000)], [y for y in range(5000)])&rsquo;
1000 loops, best of 3: 401 usec per loop</p>

<p>➜ python -m timeit &lsquo;map(lambda x,y:x+y, [x for x in range(5000)], [y for y in range(5000)])&rsquo;
100 loops, best of 3: 929 usec per loop</p>

<p>➜ python3 -m timeit &lsquo;map(lambda x,y:x+y, [x for x in range(5000)], [y for y in range(5000)])&rsquo;
1000 loops, best of 3: 397 usec per loop
```</p>

<p>实验结果中，在 Python2.7 环境下 <code>operator.add</code> 稍微快于使用 lambda 表达式，在 Python3 环境下两者几乎没有差别。事实上 Python 并不适合 CPU 密集型的应用场景，当 CPU 不是性能瓶颈时，operator 和 lambda 之间的性能差距基本可以忽略。</p>
]]></content>
  </entry>
  
</feed>
