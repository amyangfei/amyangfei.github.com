{"meta":{"title":"Amyangfei's Blog","subtitle":"Enjoy Programming","description":"我的个人博客，会在这里记录学习、工作过程中遇到的问题和对技术的思考。","author":"amyangfei","url":"http://amyangfei.me","root":"/"},"pages":[{"title":"About","date":"2020-12-13T09:06:21.886Z","updated":"2020-12-13T09:06:21.886Z","comments":true,"path":"about/index.html","permalink":"http://amyangfei.me/about/index.html","excerpt":"","text":"About meI am yangfei，来自中国北方，现居中国广州。 喜欢摄影，旅行，写代码。 Technology StackLinux, Python, Golang, C High performance web architecture, Asynchronous programming, Message queue, Data oriented architecture, System performance tuning 画外音我喜欢『 Memoirs of a Geisha 』中的一段话，生活中的得与失，静静地去体会，不喜不悲。 At the templethere is a poem called “Loss”carved into the stone.It has three words.But the poethas scratched them out.You cannot read “Loss”.Only feel it. ― Arthur Golden, Memoirs of a Geisha Contact MeGMail : amyangfei#gmail.com"},{"title":"tags","date":"2020-12-13T04:44:21.619Z","updated":"2020-12-13T04:44:21.583Z","comments":true,"path":"tags/index.html","permalink":"http://amyangfei.me/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Compare concepts described in Streaming Systems and real usage in a cdc project","slug":"streaming-systems","date":"2021-01-23T00:00:00.000Z","updated":"2021-01-24T09:42:31.402Z","comments":true,"path":"2021/01/23/streaming-systems/","link":"","permalink":"http://amyangfei.me/2021/01/23/streaming-systems/","excerpt":"I have read the first part of book Streaming Systems recently and learned quite a lot of high level concepts and many useful tips about streaming systems. Many topics that this book discusses are very instructive to the TiCDC project that I was focusing on in the past year, which is a change data capture system that supports to replicate change data from a distributed NewSQL database to various downstreams. In this article I will talk about some core concepts that a streaming system should pay attention to, comparing the opinion that the book Streaming Systems expresses, to some practical experience from the CDC project I participated. Concepts in change data capture systemThe book Stream Systems introduces many concepts to describe a streaming system from different dimensions. I will map these concepts to the change data capture system roughly.","text":"I have read the first part of book Streaming Systems recently and learned quite a lot of high level concepts and many useful tips about streaming systems. Many topics that this book discusses are very instructive to the TiCDC project that I was focusing on in the past year, which is a change data capture system that supports to replicate change data from a distributed NewSQL database to various downstreams. In this article I will talk about some core concepts that a streaming system should pay attention to, comparing the opinion that the book Streaming Systems expresses, to some practical experience from the CDC project I participated. Concepts in change data capture systemThe book Stream Systems introduces many concepts to describe a streaming system from different dimensions. I will map these concepts to the change data capture system roughly. Event time versus processing time Event time is time at which events actually occurred and processing time is the time at which events are observed in the system. In the real world, the skew between these two values always exists, and may be affected by various characteristics. Which one will be used when system processes with the event is highly related with the use scenario. As for the CDC of TiDB, we can use both the start-ts or commit-ts of a row change as event time, where start-ts and commit-ts can be treated as the prewrite and commit timestamp roughly (In fact in TiDB there exists a strict 2PC theory, and we can simplify the model that each transaction has a unique, incremental global logic timestamp, in TiDB it is called timestamp oracle, TSO for short). In TiCDC start-ts of a row change is used for data processing, with the help of event time, we can easily restore the row changed sequence in upstream, which can meet the requirement of consistent replication. Windowing Windowing is the notion of taking a data source (either unbounded or bounded), and chopping it up along temporal boundaries into finite chunks for processing. Window is a common approach used to cope with unbounded data, in my opinion it can be a logic concept that tries to split data into determinate boundaries. The way how a streaming system cuts data into windows leads to different data processing strategies. In TiCDC, window is more like an adaptive size window, where the size is determined by the watermark and trigger. Triggers A trigger is a mechanism for declaring when the output for a window should be materialized relative to some external signal. The streaming system can contain multiple components and forms a data flow chain, each component has an upstream and a downstream. Here materialized means output to the downstream of the current component. Trigger is the decision maker of when to output for a window, the choice of trigger will affect the processing quality in many aspects, including latency, correctness, throughput etc. In real world, streaming data is not stable all the time, there could exist late data, traffic peak, data re-delivery, data source panic and recovery etc. All these abnormal conditions should be taken into consideration when we design a proper trigger mechanism. In TiCDC, the trigger mechanism is highly relying on the watermark, which is a perfect watermark that can ensure a window closed. Since the transaction sequence in the upstream must be kept in TiCDC (which will enable sink or downstream consumer to recover transaction and keep data consistent), TiCDC adopts the same mechanism used in CockroachDB CDC, which is also the theory from paper Naiad: A Timely Dataflow System. The core idea of this mechanism is the trigger in each component can’t materialize data that is later than the perfect watermark to downstream, in another word, in order to output data best effort, the trigger should materialize all data before the perfect watermark. Watermarks Watermarks are temporal notions of input completeness in the event-time domain. Worded differently, they are the way the system measures progress and completeness relative to the event times of the records being processed in a stream of events. Depending upon the type of watermark, perfect or heuristic, that assertion may be a strict guarantee or an educated guess, respectively. How to choose a suitable watermark is decided by the usage scenario, which is also a tradeoff between performance and correctness as well as scalability and availability. Supposing we have a system that requires low latency of streaming data subscription, can tolerate with late data (data out of the scope of a closed window, tolerate means the system can ignore the late data or has method to amend correctness with late data), then a heuristic watermark is better than a perfect watermark. In another situation, if a system requires to keep the event sequence strictly, such as a database replication system must keep transaction serializable when replicating to downstream (Transaction isolation is a complicated topic in database system, here serializable is not the same thing as the serializable isolation level, but means the transaction applied sequence in downstream is the same as the event time sequence in upstream. Yet this serializable could be strict or loose, such as table level serializable or transaction level serializable), then the heuristic watermark could lead to inconsistent state between upstream and downstream, if this inconsistency is not acceptable, perfect watermark is a better choice than heuristic watermark. Accumulation An accumulation mode specifies the relationship between multiple results that are observed for the same window. There exists multiple ways to refine or relate results, including discarding, accumulating or accumulating and retracting. Different accumulations have different semantics, and will output different results with the same window/watermark strategy. However in CDC no specific accumulation is used, since the row changed event in CDC is atomistic, the only existing results relate maybe transaction oriented events grouping, which means events in the same window can be grouped by transaction identification(events with the same start-ts will be treated in a same transaction). Tradeoffs in streaming systemsIn this part I will focus on some key indicators in streaming systems and how to make a good tradeoff among these indicators. Latency and precisionIn theory, the strategy with watermark has a direct impact on latency. Besides the stages of data flow and how watermarks propagating is implemented also have a big impact on latency. As for precision, it is mostly decided by watermark mechanism. As discussed in part one, perfect watermark can ensure a 100% precision of results materializing, well perfect watermark is impractical for many real world distributed input sources. However in some known scenarios, it is possible to define a perfect watermark, such as Distributed system with a single, monotonically increasing timestamp allocator (The TSO in TiDB and HLC in CockroachDB are all this mechanism) A statically sized input source of time-ordered input, such as Apache Kafka topic with a static set of partitions, each partition of source can be consumed with monotonically increasing event time. If a streaming system adopts the above-mentioned perfect watermark, the latency is determined by the latency of input source. Supposing there exist N nodes in input source, and the logical timestamp forwards to T, the streaming system must wait for each of the N nodes has sent event equal or greater than T. Based on this point, if the upstream input source suffers an accident, saying partial of N nodes crash and restart, the events of these fault nodes will be delayed and latency in streaming system will be increased remarkably until the upstream recover. In the real world most upstreams have some rebalance strategies when disaster happens, the data flow of down nodes will be transferred to normal nodes to decrease the recovery time, which is also known as RTO in distributed systems. To decrease latency as far as possible, the streaming system must be awareness to upstream fault recovery or rebalance, however the latency lag won’t be less than the RTO in upstream. In real world most streaming systems contain multiple stages, watermarks are propagated across each independent stage. As for each stage, it has an input watermark and an output watermark, the output watermark is later than input watermark and the latency is contributed by the processing time of this stage. So it is obvious that the more stages in a pipeline, the more latency will be gained. As for each stage, the processing is not always monotonic because we can segment the processing within one stage into a flow with several conceptual components, each of which contributes to the output watermark. This is a common optimization, which aims to decrease processing time by dividing jobs into parallel sub jobs. Latency and throughputLatency and throughput sometimes look like two incompatible things, however high throughput as well as low latency are often declared to be features that are provided by many streaming systems. So what is the real situation? To clarify this complicated problem, we must have a clear definition of latency and throughput, and have a knowledge about how these metrics affect the performance a streaming system. The paper Benchmarking Distributed Stream Data Processing Systems which is published in 2018 ICDE has a good explanation about key indicators of distributed streaming systems, it has two types for both latency and throughput, event-time latency, processing-time latency, maximum throughput and sustainable throughput respectively, the finer granularity of latency measure, the better observability can gain of a system, which also makes it easier for message tracking and stuckness diagnosis. In fact both latency and throughput are in the performance scope, whether they interact each other has many factors, a good designed streaming system should be able to achieve a liner sustainable throughput along with extensible processing nodes, provide a reasonable latency for data processing and materializing, and what’s more, the throughput and latency can be tuned (such as increasing throughput with acceptable latency increasing, or decreasing acceptable throughput to reduce latency) based on the requirement. Performance and delivery guaranteeGuaranteeing fault-tolerant and performant stream processing is hard because most streaming processing is stateful (which means it is expensive to replay from some old points) and distributed (which means node failure happens a lot). Due to the complexity of the problem, there are many approaches to fault tolerance in the open source ecosystem. Such as the following strategies used by open source projects compared in this article. Record acknowledgements (Apache Storm) Micro batching used (Apache Storm Trident, Apache Spark Streaming) Transactional updates (Google Cloud Dataflow) Distributed Snapshots (Apache Flink) In the real implementation of TiCDC, we choose at least once delivery strategy, for two reasons Sink is robust in the face of replay: For the sink type of message queue, TiCDC can output a perfect watermark, in each sized window with watermark, data integrity is guaranteed even redundant data exists; For the sink type of relation database, TiCDC can output idempotent SQL statements event data is re-delivery. We adopt a checkpoint mechanism in sink, which means sink maintains a global output watermark and saves it into persistent storage periodically, with the checkpoint we can replay whenever error happens, which makes the system fault tolerable and fast recoverable. Combining with the checkpoint mechanism, the system can achieve better performance, since the loose of delivery guarantee makes it easy to use batch mechanism and fast delivery strategy. SummaryTo design a general streaming system is tough stuff, mostly because the workflow and data processing pattern can be various and complex. And in many scenarios, a general streaming system may not work better than a customized system, how to choose an appropriate framework is always controversial in software engineering. This article just has a limited view of streaming systems, the evolution of streaming architectures never stops, what we should do is to keep learning, thinking and practising.","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"streaming system","slug":"streaming-system","permalink":"http://amyangfei.me/tags/streaming-system/"},{"name":"change data capture","slug":"change-data-capture","permalink":"http://amyangfei.me/tags/change-data-capture/"},{"name":"book reading notes","slug":"book-reading-notes","permalink":"http://amyangfei.me/tags/book-reading-notes/"}]},{"title":"Thinking about etcd lease","slug":"thinking-about-etcd-lease","date":"2020-12-27T00:00:00.000Z","updated":"2021-01-03T02:54:34.912Z","comments":true,"path":"2020/12/27/thinking-about-etcd-lease/","link":"","permalink":"http://amyangfei.me/2020/12/27/thinking-about-etcd-lease/","excerpt":"This article will first talk about a wrong use case with etcd lease, and then based on that case, I will dig into the design principle of etcd lease and some use scenario with etcd lease. A wrong use case with etcd leaseThere exist two roles in the following scenario, one is client and the other one is coordinator. More than one clients could exist at the same time. Each of them has an unique ID, and when a new client starts it applies a new lease from etcd and puts a key corresponding to the client ID with lease as option. The client must call KeepAlive of its lease periodically to keep lease not timeout. Once the lease is timeout and deleted by etcd server, the client becomes illegal and should not access the etcd resource anymore. The coordinator monits the client ID key, when a new client registers, it allocates new resource/task to this client, which can be represented by a client ID relevant key value. And when it detects client ID key deleted (which means the client lease timeout), it will recycle the allocated resource of this client. Here we use a etcd session to maintain client lease and lease keepalive, a simple work model of client is as follows","text":"This article will first talk about a wrong use case with etcd lease, and then based on that case, I will dig into the design principle of etcd lease and some use scenario with etcd lease. A wrong use case with etcd leaseThere exist two roles in the following scenario, one is client and the other one is coordinator. More than one clients could exist at the same time. Each of them has an unique ID, and when a new client starts it applies a new lease from etcd and puts a key corresponding to the client ID with lease as option. The client must call KeepAlive of its lease periodically to keep lease not timeout. Once the lease is timeout and deleted by etcd server, the client becomes illegal and should not access the etcd resource anymore. The coordinator monits the client ID key, when a new client registers, it allocates new resource/task to this client, which can be represented by a client ID relevant key value. And when it detects client ID key deleted (which means the client lease timeout), it will recycle the allocated resource of this client. Here we use a etcd session to maintain client lease and lease keepalive, a simple work model of client is as follows 12345678910session, err := concurrency.NewSession(etcdCli, concurrency.WithTTL(10))for &#123; select &#123; case &lt;-session.Done(): // lease is timeout, client exits return case ev := &lt;-etcdCli.Watch(ctx, resourceKey): // process resource update, mainly some etcd key value operations &#125;&#125; In the above code, session.Done is used for lease aliveness check. However this is not a strict aliveness protection for resource access for two reasons. After client receives a new update from watch chan, during the process procedure of resource update, the client’s lease could be timeout, which means the client could still access the resource after it is illegal. The session.Done for a lease is not triggered in real-time, which means when the lease is timeout and revoked by etcd server, the session.Done channel may not be fired immediately. This is because session.Done is only notified after the etcd client establish a new keepalive request, there could be a time window as long as 1/3 of session ttl that session.Done is not notified. The goal of aliveness guarantee for resource access can be achieved by using etcd Txn simply. As there exists a key bounded with client lease, the client can make use of this key to guarantee lease is timeout during other etcd key value operations. The main logic can be as follows. 123456789101112131415161718192021222324ErrLeaseTimeout := errors.New(&quot;lease associated key is deleted&quot;)func alivenessGuaranteeAccess(ctx context.Context, leasedKey, resourceKey string) error &#123; resp, err := cli.Get(ctx, leasedKey) if err != nil &#123; return err &#125; if resp.Count == 0 &#123; return ErrLeaseTimeout &#125; revision := resp.Kvs[0].ModRevision txnResp, err := etcdCli.Txn(ctx).If( clientv3.Compare(clientv3.ModRevision(leasedKey), &quot;=&quot;, revision), ).Then( clientv3.OpPut(resourceKey, &quot;new value&quot;), ).Commit() if err != nil &#123; return err &#125; if !txnResp.Succeeded &#123; return ErrLeaseTimeout &#125; log.Info(&quot;update resource key successfully&quot;) return nil&#125; Dig into the implement principle of etcd leaseIn this part I will talk about the implement principle of etcd lease based on code in tag-3.4.14. Basically, each etcd server runs a lease manager which implements the Lessor interface. Most of the lease management is via raft to keep lease information consistent among multiple etcd servers. Take the lease grant operation as an example. When a LeaseGrantRequest is received by etcd server, the gRPC request will be processed in LeaseGrant of a lease server and return LeaseRevokeResponse after processing. When processing the LeaseGrantRequest, it will be passed to LeaseGrant function of EtcdServer/Lessor to trigger an internal raft request. Then raft message will be applied via the internal raft mechanism to all servers. When applying the LeaseGrant message in each etcd server, The Grant function of a Lessor will be finally called. The main event loop of a Leasor contains two periodic jobs, revokeExpiredLeases and checkpointScheduledLeases, both of them run every 500ms. revokeExpiredLeases finds all leases past their expiry and sends them to an expired channel for revoking, the channel is consumed in etcd server’s main loop. Each lease is associated with a LeaseItem and all lease items are stored in a min heap, the heap item is sorted by the expiration time of lease. When I was reading the code about iterating the expiration heap, I found an interesting code snippet, each time the lessor pops an expired item from the heap, it will put back a new lease item with the same lease ID but adding an expiredLeaseRetryInterval to the expired time. This is a patched logic to fix a bug that if the receiver of expired channel does not revoke lease successfully, the lease will be never revoked because it can’t be retrieved from lease expiration heap anymore. More details can be found in this PR. checkpointScheduledLeases was introduced since etcd 3.4 in this PR, this PR has described the requirement and mechanism of lease checkpointing detailedly. It is designed for the scenario that one etcd leader is transfered, the new leader will rebuild lease information and inherit the remaining ttl of existing leases instead of auto-renew to their full TTL. Precision of etd leaseIn short, the precision of etcd lease is second level, which is reflected in two aspects: When a lease is granted, the time unit for ttl is second. Besides there exists a minimum ttl mechanism in etcd. Since etcd server uses a lazy way to determine which lease is timeout, instead of some more precise notification mechanism, it adds a latency for lease timeout. This means when we grant a new lease with TTL = N second, and don’t send any keepalive request for this lease, the time window that this lease will be revoked in etcd server is about [N, N + delta second], where delta is generally 0.5, but considering some time cost of other logic, the delta could be more than 0.5. Taking a sample code as example, this code snippet grants a new lease with TTL=5s every 50ms, 20 leases totally. For each lease attaches a key on it and sends a keepalive request to etcd server to refresh lease. Then watches for the key delete operation and records the duration for each lease timeout. From the testing result, the duration of lease revoked is between [5s, 5.6s], which is as expected. What’s more, etcd server has a hard code limit when revoking lease, each round of expired lease revoking, at most 500 leases can be revoked. This can be easily verified by the code snippet. In this scenario the lease expiry duration will have more latency, a test result is as follows: duration(s) 5 5.1 5.2 5.3 5.4 5.5 5.7 5.8 6 6.1 6.4 6.5 6.8 6.9 7.2 7.3 7.7 8.1 8.2 lease count 23 2 40 147 346 29 470 30 125 375 1 499 21 479 1 499 500 265 148 In most cases, making a large amount of keys expire at the same time is not a good design. And when we use etcd lease, we must be aware of the lazy expiration mechanism. Tolerance with clock driftOperating systems provide both a “wall clock” which is subject to changes for clock synchronization, and a “monotonic clock” which is not. The general rule is that the wall clock is for telling time and the monotonic clock is for measuring time. Is the etcd lease reliable if the system’s wall clock is updated by NTP service? The answer is yes, both in the etcd server side and etcd client side, the lease implementation is reliable because monotonic clock is used. Since Go 1.9 builtin monotonic time library is provided, etcd makes use of this feature to ensure the safety of time comparison. For the server side, both the expiry time setter of a lease and expired checker are using monotonic time. For the client side, it uses Time.Before() API to check whether a keepalive request should be sent, which is also clock drift tolerable. SummaryEtcd lease is powerful but has some restrictions, it is better to know the underlying principle of etcd lease, which will help to use it correctly and reasonably.","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://amyangfei.me/tags/golang/"},{"name":"etcd","slug":"etcd","permalink":"http://amyangfei.me/tags/etcd/"}]},{"title":"Some best practices with go etcd library","slug":"best-practice-with-go-etcd","date":"2020-12-19T00:00:00.000Z","updated":"2021-01-03T03:01:49.628Z","comments":true,"path":"2020/12/19/best-practice-with-go-etcd/","link":"","permalink":"http://amyangfei.me/2020/12/19/best-practice-with-go-etcd/","excerpt":"Etcd is a distributed reliable key-value store written by golang and native golang client binding is provided in its official repository, which makes it very convenient as well as robust to write golang code to communicate with etcd server. Here robust means the etcd client should guarantee correctness and high availability under faulty conditions, more details can be found in etcd official client design document. This article summarizes some best practises with etcd client library from the lessons I learned from the production environment. To explain some implementation mechanisms I will also link to etcd source code (mainly based on branch release-3.4). Consume data from the watch response channel ASAPWatch works with a bi-directional gRPC stream between watch client and etcd server, the most common way to use watch is as follows, receive keyspace changed result from the watch channel and consume these KV events one by one.","text":"Etcd is a distributed reliable key-value store written by golang and native golang client binding is provided in its official repository, which makes it very convenient as well as robust to write golang code to communicate with etcd server. Here robust means the etcd client should guarantee correctness and high availability under faulty conditions, more details can be found in etcd official client design document. This article summarizes some best practises with etcd client library from the lessons I learned from the production environment. To explain some implementation mechanisms I will also link to etcd source code (mainly based on branch release-3.4). Consume data from the watch response channel ASAPWatch works with a bi-directional gRPC stream between watch client and etcd server, the most common way to use watch is as follows, receive keyspace changed result from the watch channel and consume these KV events one by one. 123456789101112131415161718cli, err := clientv3.new(clientv3.config&#123; endpoints: []string&#123;&quot;127.0.0.1:2379&quot;&#125;, context: ctx, dialtimeout: 3 * time.second, dialoptions: []grpc.dialoption&#123;&#125;,&#125;)// handle errorch := cli.Watch(ctx, &quot;/some/keyspace&quot;, clientv3.WithPrefix())for resp := range ch &#123; for _, event = range resp.Events &#123; switch event.Type &#123; case mvccpb.PUT: // process with put event case mvccpb.DELETE: // process with delete event &#125; &#125;&#125; Supposing the process throughput for changed KV events is less than the watched keyspace changed frequency, it is easy to find the delay between consumption and keyspace update in etcd, with the help of monitoring system or some data comparison, etc. If we can’t reduce the update frequency of etcd keyspace, then we must ensure the process throughput can match the keyspace update frequency, by either increasing the consume speed of consumer or cache the KV changed events from watch channel and consume data asynchronously. There exist other ways to use watch, for example, multiple clients change the same key randomly, in an atomic way by using Txn API. Another client wants to subscribe to the update event of this key as soon as possible, but when it knows the key has changed, it doesn’t need to know data for each MVCC version exactly. Such as three clients, each of them updates the key once, and the key has three versions: v1, v2, v3. The subscribe client wants to know the key is updated and queries the data in v3 is enough. The logic for subscribe client can be as follows. It creates a channel to receive keyspace change notification from the watch channel and the outer code consumes this channel continually. 1234567891011121314151617181920212223242526output := make(channel struct&#123;&#125;, 1)go func() &#123; ctx, cancel := context.WithCancel(parentCtx) ch := cli.Watch(ctx, &quot;/some/key&quot;) for resp := range ch &#123; if resp.Err() != nil &#123; // error processing break &#125; select &#123; case &lt;-ctx.Done(): case output &lt;- struct&#123;&#125;&#123;&#125;: &#125; &#125; cancel()&#125;()for &#123; select &#123; case &lt;-parentCtx.Done(): return case &lt;-output: // some logic code here &#125;&#125; This is a real production scenario and the subscribe client works well (But in fact it is not accurate, I will explain later), and we can always get the keyspace update notification in time. However we observed a slowly increment with heap memory, and a heap pprof shows large amount of inuse memory with go.etcd.io/etcd/mvcc/mvccpb.(*KeyValue).Unmarshal and go.etcd.io/etcd/clientv3.(*watchGrpcStream).dispatchEvent. After simplifying the working model to the above code we find out the root cause is consume throughput of output channel is less then keyspace update frequent, which is the same thing as the first scenario! Since the buffer size of output channel is 1, KV changed events are accumulated in etcd client, and the notification from output channel is out of date, the subscribe client works well is just because the keyspace is keeping updated, but the notification is out of date already. The fix for this scenario is quite simple, we change the notification to a non-blocking way by adding a default branch in select. 12345 select &#123;- case &lt;-ctx.Done(): case output &lt;- struct&#123;&#125;&#123;&#125;:+ default: &#125; From etcd source code we can know each watcherStream holds an unlimited WatchResponse buffer, and the watched result is appended to this buffer no matter how large it is. So if the watched keyspace keeps changing, and the consume speed from the watch channel is slow, large amount of WatchResponse objects will be accumulated in etcd client. This is the lesson we learn from this case, when using etcd watch, we should consume data from the watch response channel as soon as possible in order to keep the keyspace change notification up to date, and prevent unnecessary memory consumption. Besides if a watch client just wants to subscribe to the keyspace change notification, but doesn’t care about the content of key and value, is there any OpOption in etcd client library to ignore key or value. Unfortunately, there is no such option for watch API in golang etcd library currently. Tips for using a lease SessionLease is a mechanism for detecting client liveness in etcd. The etcd golang library defines an Lease interface and provides a KeepAlive API which will try to keep the given lease alive forever. The KeepAlive API returns a *LeaseKeepAliveResponse chan, we can read from this channel to know the latest TTL for given leaseID. The common way to create a new Lease and set keepalive for it is as follows. 123456789101112131415// ignore error handlingcli, err := clientv3.New(clientv3.config&#123;endpoints: []string&#123;&quot;127.0.0.1:2379&quot;&#125;&#125;)leaseResp, err := cli.Grant(ctx, 10 /*ttl*/)ch, err := cli.KeepAlive(ctx, leaseResp.ID)go func() &#123; for &#123; select &#123; case &lt;-ctx.Done(): return case resp := &lt;-ch: // process lease response &#125; &#125;&#125;() Note the response channel size of KeepAlive has a buffer size of 16, if the channel is not consumed promptly the channel may become full. After channel is full, the lease client will continue sending keepalive requests to the etcd server, but will drop responses until there is capacity on the channel to send more responses. If we don’t want to maintain the keepalive for a lease manually, the etcd golang library also provides a powerful Leased Session encapsulation. When creating a session via NewSession API, it grants a new Lease and setup KeepAlive for the binding client automatically, more details can be found in the etcd source code. Lease TTL is the most important option when we interact with Lease, it is necessary to know how keepalive works with TTL. The first question is how often will a client send the keepalive request to etcd server. It is easy to find in source code, the lessor uses a time barrier mechanism to determine when to send a keepalive message. For each time a keepalive response is received, the lessor sets the nextKeepAlive time to 1/3 of keepalive TTL. After each round of keepalive request the sendKeepAliveLoop will sleep 500ms before the next keepalive round. There is another question, what will happen when network is not stable between etcd client and etcd server? In this scenario the keepalive request message could be blocked because there is no sufficient flow control to schedule messages with the gRPC transport, and the lease could be timeout, the timeout will be detected in the deadlineLoop. Lease TTL should be set carefully, according to the discussion in github, the minimum TTL needs to be larger than the election timeout of etcd server, which is 1s by default, and can be 3-5s usually. What’s more, etcd server has a minimum lease TTL restriction, any requests for shorter TTLs will be extended to the minimum TTL. The value of minimum lease TTL is based on election-timeout of etcd server, and it equals to math.Ceil(3 * election-timeout / 2)(ref). For example, if election-timeout is 3s, the minimum lease TTL is math.Ceil(3 * 3 / 2) = 5s. SummaryThe etcd source code has good readability, detail comments and full completed test cases, and I have also learned many use tips from the etcd source code. If you also use etcd golang library in your code, I strongly recommend reading the source code of etcd to know exactly how it works and what we need to pay attention to.","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"source code reading","slug":"source-code-reading","permalink":"http://amyangfei.me/tags/source-code-reading/"},{"name":"golang","slug":"golang","permalink":"http://amyangfei.me/tags/golang/"},{"name":"etcd","slug":"etcd","permalink":"http://amyangfei.me/tags/etcd/"}]},{"title":"使用 Go 编写 Redis Loadable Modules","slug":"redis-module-with-cgo","date":"2016-08-03T00:00:00.000Z","updated":"2020-12-13T07:06:58.025Z","comments":true,"path":"2016/08/03/redis-module-with-cgo/","link":"","permalink":"http://amyangfei.me/2016/08/03/redis-module-with-cgo/","excerpt":"本文会介绍如何使用 go 编写 redis loadable modules，并分析编写模块和使用cgo可能遇到的坑。 什么是redis loadable modules可加载模块是redis最新加入的功能，目前需要在unstable分支才可以使用。简单说模块系统是redis的C代码暴露出一些API，定义在头文件redismodule.h中，外部模块引用该头文件即可访问所有的API函数，这些API提供了包括访问redis的字典空间、调用redis命令、向客户端返回数据等诸多功能。外部模块是以动态库的形式被redis server加载并使用，可以在redis server启动时加载，也可以在启动后动态加载。更多的细节可以参考文档redis module INTRO。 在此之前想对redis扩展有两种方案：一是利用lua脚本；另一种则需要修改redis源码，类似于Kit of Redis Module Tools提供的方案。lua脚本的扩展性有限，并且lua是通过redis的上层API来调用redis命令的，无法直接访问底层的存储数据，调用redis更底层的API；修改源码的方案就更加hack，是没有办法不断与上游分支合并的。","text":"本文会介绍如何使用 go 编写 redis loadable modules，并分析编写模块和使用cgo可能遇到的坑。 什么是redis loadable modules可加载模块是redis最新加入的功能，目前需要在unstable分支才可以使用。简单说模块系统是redis的C代码暴露出一些API，定义在头文件redismodule.h中，外部模块引用该头文件即可访问所有的API函数，这些API提供了包括访问redis的字典空间、调用redis命令、向客户端返回数据等诸多功能。外部模块是以动态库的形式被redis server加载并使用，可以在redis server启动时加载，也可以在启动后动态加载。更多的细节可以参考文档redis module INTRO。 在此之前想对redis扩展有两种方案：一是利用lua脚本；另一种则需要修改redis源码，类似于Kit of Redis Module Tools提供的方案。lua脚本的扩展性有限，并且lua是通过redis的上层API来调用redis命令的，无法直接访问底层的存储数据，调用redis更底层的API；修改源码的方案就更加hack，是没有办法不断与上游分支合并的。 显然心的模块系统明显优于以上两种方案，优点包括： 直接访问redis存储的各种数据结构； 直接对存储数据的内存进行操作； 模块仅依赖redismodule.h暴露的接口函数，而不依赖redis本身的实现，因此可以兼容redis的版本升级。 模块系统也有缺点，比如模块中的代码bug引发的异常会直接导致redis server crash掉；模块的问题譬如引入的内存泄漏，代码执行阻塞都会影响redis服务的正常运行。这些缺点不是模块系统本身的问题，而是这种扩展的灵活性和系统稳定性的权衡，是可以通过优质的扩展模块来避免的。 使用C来编写redis扩展模块很简单，参考文档redis module INTRO，你可以在5分钟内学会编写一个redis扩展模块。Redis Lab官方也提供了很多有趣的模块 Module Hub。在文档中同样提到可以用其它语言来编写redis扩展模块。 it will be possible to use C++ or other languages that have C binding functionalities. go语言的cgo提供了Go和C互相调用的支持，因此本文来尝试通过go语言编写redis的扩展模块。下文所有的代码都可以在RedisModules-Go这个仓库找到，仓库里还提供了redis module lab的两个模块password，graphicsmagick的go版本，以及一些简单的benchmark。 redis扩展模块的形式是很固定的，需要编写且只编写两个部分：注册命令的函数和具体实现命令的函数。我们分两个阶段来编写redis的go扩展模块：第一阶段是通过go编写逻辑代码，即go代码拿到数据、处理、返回处理结果；第二阶段是go代码直接访问redismodule.h提供的API获取数据、处理、返回。先看看第一种类型。 通过go编写逻辑处理基本思路是使用go编写逻辑处理，go的函数接收的输入是C的数据类型，可以是指向C内存空间的指针；在C代码中调用由go编写的逻辑处理函数，具体的调用方式是go代码编译时指定buildmode=c-shared得到动态库和相关头文件，在C代码中引用头文件并调用。最简单的实现代码如下所示： 123456789101112131415/* ECHO1 &lt;string&gt; - Echo back a string sent from the client */int Echo1Command(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) &#123; if (argc &lt; 2) return RedisModule_WrongArity(ctx); RedisModule_AutoMemory(ctx); size_t len; char *dst = RedisModule_Strdup(RedisModule_StringPtrLen(argv[1], &amp;len)); struct GoEcho1_return r = GoEcho1(dst); RedisModuleString *rm_str = RedisModule_CreateString(ctx, r.r0, r.r1); free(r.r0); RedisModule_Free(dst); RedisModule_ReplyWithString(ctx, rm_str); return REDISMODULE_OK;&#125; 123456789101112package main// #include &lt;stdlib.h&gt;import &quot;C&quot;//export GoEcho1func GoEcho1(s *C.char) (*C.char, int) &#123; gostr := (C.GoString(s) + &quot; from golang1&quot;) return C.CString(gostr), len(gostr)&#125;func main() &#123;&#125; 具体实现中涉及到申请的内存空间是在C的运行环境还是go的运行环境，因为go的运行时提供了gc，而C则需要手动管理内存，因此这其中有很多细节需要注意；同时go和C之间可以传递的数据也有一些限制，后文会详细叙述。 通过go调用redismodule.h定义的API第一步中所做的是提供数据给go代码进行逻辑处理并返回数据，那么如果希望在go代码中也可以调用redismodule.h定义的API，又需要如何处理？ 直观的想通过go调用C代码，在go中直接#include &quot;redismodule.h&quot;就可以了嘛，于是我们编写如下的go代码： 123456789101112131415161718192021package main// #include &quot;redismodule.h&quot;/*typedef RedisModuleString *(*redis_func) (RedisModuleCtx *ctx, char *ptr, size_t len);inline RedisModuleString *redis_bridge_func(redis_func f, RedisModuleCtx *ctx, char *ptr, size_t len)&#123; return f(ctx, ptr, len);&#125;*/import &quot;C&quot;//export GoEchofunc GoEcho(ctx *C.RedisModuleCtx, s *C.char) *C.RedisModuleString &#123; gostr := (C.GoString(s) + &quot; from golang&quot;) f := C.redis_func(C.RedisModule_CreateString) return C.redis_bridge_func(f, ctx, C.CString(gostr), C.size_t(len(gostr)))&#125;func main() &#123;&#125; redismodule.h定义的API函数都是函数指针，由于go不支持直接调用C的函数指针，所以通过通过go的变量保存C的函数指针，并将该变量作为参数调用C的bridge_function，在bridge_function中调用目标函数指针。编译是可以通过的，但实际运行就会crash。通过调试很容易发现C.RedisModule_CreateString的值是nil，它没有指向正确的函数地址。那换一种方式使用一个C的函数来调用RedisModule_CreateString呢？ 1234567891011121314151617181920212223package main/*#include &quot;redismodule.h&quot;inline RedisModuleString *RedisModule_CreateString_Wrap(RedisModuleCtx *ctx, char *ptr, size_t len) &#123; void *getapifuncptr = ((void**)ctx)[0]; RedisModule_GetApi = (int (*)(const char *, void *)) (unsigned long)getapifuncptr; RedisModule_GetApi(&quot;RedisModule_CreateString&quot;, (void **)&amp;RedisModule_CreateString); RedisModuleString *rms = RedisModule_CreateString(ctx, ptr, len); return rms;&#125;*/import &quot;C&quot;//export GoEchofunc GoEcho(ctx *C.RedisModuleCtx, s *C.char) *C.RedisModuleString &#123; gostr := (C.GoString(s) + &quot; from golang version bridge&quot;) return C.RedisModule_CreateString_Wrap(ctx, C.CString(gostr), C.size_t(len(gostr)))&#125;func main() &#123;&#125; 实际运行时依然会在调用RedisModule_CreateString函数时crash掉，跟进gdb调试可以看到该函数指针并没有指向正确的内存地址。 12345678910111213(gdb) l23 /*4 #include &quot;redismodule.h&quot;56 inline RedisModuleString *RedisModule_CreateString_Wrap(RedisModuleCtx *ctx, char *ptr, size_t len) &#123;7 RedisModuleString *rms = RedisModule_CreateString(ctx, ptr, len);8 return rms;9 &#125;10 */11 import &quot;C&quot;(gdb) p RedisModule_CreateString$1 = (RedisModuleString *(*)(RedisModuleCtx *, const char *, size_t)) 0x0 那么redismodule.h对外提供的函数指针是在何时指向实际的函数内存地址呢？从redismodule.h文件本身就会得到答案：加载一个外部模块都需要调用RedisModule_Init函数，在这个函数中会通过RedisModuleCtx *ctx变量定位到在redis代码module.c内提供的实际API函数，然后将函数指针指向真正的函数地址。由于在上述的go模块中，没有调用RedisModule_Init，RedisModule_CreateString自然指向的是非法的内存地址。所以简单改动一下代码，主动注册一下函数指针的地址即可在go的模块中调用redismodule.h提供的API。 1234567891011121314151617181920212223package main/*#include &quot;redismodule.h&quot;inline RedisModuleString *RedisModule_CreateString_Wrap(RedisModuleCtx *ctx, char *ptr, size_t len) &#123; void *getapifuncptr = ((void**)ctx)[0]; RedisModule_GetApi = (int (*)(const char *, void *)) (unsigned long)getapifuncptr; RedisModule_GetApi(&quot;RedisModule_CreateString&quot;, (void **)&amp;RedisModule_CreateString); RedisModuleString *rms = RedisModule_CreateString(ctx, ptr, len); return rms;&#125;*/import &quot;C&quot;//export GoEchofunc GoEcho(ctx *C.RedisModuleCtx, s *C.char) *C.RedisModuleString &#123; gostr := (C.GoString(s) + &quot; from golang version bridge&quot;) return C.RedisModule_CreateString_Wrap(ctx, C.CString(gostr), C.size_t(len(gostr)))&#125;func main() &#123;&#125; 一些细节编写模块的过程中关于内存使用发现不少有趣的地方。 1. Go和C之间传递指针的规则和限制在go1.5/hello_module.go这个文件中可以看到export给C的go函数的返回值很多返回了go的指针，在C代码中调用go函数之后可以直接访问go的内存，这在go1.5是支持的，但是从go1.6之后加入了Go和C之间传递指针的限制，明确指出： A Go function called by C code may not return a Go pointer. go1.6对go和C的互相调用进行了大量的规范，有编译层面的也有在运行时的检查，详细可以参考12416-cgo-pointers。规范的出发点有两点：一是更便于go的内存管理，总所周知go提供了gc，所以有一些破坏了gc规则的使用需要禁止；二是尽可能减少程序运行时内存访问出现的未知错误。 那么回到我们的redis go模块，如果不能从go函数返回go指针给C代码使用，那么从go如何返回数据给C代码呢？目前有两种，一是返回完整的C数据，二是返回C的指针。以hello.echo模块为例，对应的两种返回形式分别是： 1234567891011func GoEcho1(s *C.char) (*C.char, int) &#123; gostr := (C.GoString(s) + &quot; from golang1&quot;) return C.CString(gostr), len(gostr)&#125;func GoEcho6(s *C.char, length, capacity C.int) (unsafe.Pointer, int) &#123; incr := &quot; from golang6&quot; zslice := cgoutils.ZeroCopySlice(unsafe.Pointer(s), int(capacity), int(capacity), false) copy(zslice.Data[int(length):], incr) return unsafe.Pointer(&amp;zslice.Data[0]), int(length) + len(incr)&#125; echo1就是直接通过C.String生成C的char *并进行一次内存复制，将gostr的内容复制到C的内存空间。echo6的例子则是现将从C代码传来的内存地址直接映射到go的slice（这里在C代码已经为待处理的数据申请了足够多的内存空间），然后直接在go中直接操作这部分内存，最后返回这部分内存的C指针。显然，echo6比echo1减少了gostr的一次内存复制。使用较大的echo string进行benchmark可以很明显看到echo6比echo1更快。 12Benchmark_Echo1 2000 815435 ns/opBenchmark_Echo6 2000 575206 ns/op 2. 内存管理在使用cgo时一定需要区分变量是在go的内存空间还是在C的内存空间，尤其注意在go代码中申请的C的数据一定需要手动释放内存，因为go的gc并不会回收这部分内存。所以譬如通过C.Cstring()生成的数据、通过C.malloc()申请的内存在使用后都需要手动回收。 3. Go直接映射C的内存空间在go代码中C的数据类型会映射成为C.x，可以直接访问C数据类型的变量，但是如果想要对变量进行go代码的逻辑操作，就必须先转换成为go的数据类型，譬如希望对一个C的char *进行处理，需要现转换成为go的slice。转换有两种方法：一是直接调用C.GoBytes方法，另一种是利用反射构造slice，具体的使用方法参考sharemem.go。使用C.GoBytes会将C数据结构的内存复制到go的内存空间。而第二种方法则是直接映射，没有内存复制。 在直接映射C内存的使用场景下，对go对象slice的一些操作可以直接作用于C的内存空间，譬如在echo6中使用copy(zslice.Data[int(length):], incr)。这里就需要注意，如果一开始C申请的内存空间不足，但是强制增大go中slice的capacity，然后进行copy操作，结果会发现程序运行中会crash掉。这种用法的问题在于不能区分slice增加的capacity内存是位于C代码还是go代码的运行环境；如果不是copy而是使用slice的append操作，由于capacity不足go的slice会重新申请内存空间，那么对应的内存都位于go的运行环境，与原来C运行环境中的数据就没有关系了。 小结后续还会对比使用go和C编写同样的扩展模块在性能方面会有怎样的差距，这里就不再继续讨论。redis计划会在4.0版本中合并模块系统的功能，这还是很值得期待的。使用cgo来完成go和C的交互也十分常见，go标准库本身就有很多地方使用到了cgo来使用C的代码，本文编写的graphicsmagick模块使用的imagick库本身也是通过cgo对ImageMagick MagickWand C API的封装。","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://amyangfei.me/tags/golang/"},{"name":"redis","slug":"redis","permalink":"http://amyangfei.me/tags/redis/"},{"name":"redis module","slug":"redis-module","permalink":"http://amyangfei.me/tags/redis-module/"}]},{"title":"RQ (Redis Queue) 使用的一些思考","slug":"redis-queue-rethink","date":"2016-01-30T00:00:00.000Z","updated":"2020-12-13T07:02:34.407Z","comments":true,"path":"2016/01/30/redis-queue-rethink/","link":"","permalink":"http://amyangfei.me/2016/01/30/redis-queue-rethink/","excerpt":"最近使用了 rq 这个简单的队列处理库，其中有一些任务需要使用MySQL的连接或者redis的连接，对此有一些思考。 MySQL/redis的连接复用rq 提供了两种 worker 模型：基于 fork 的 worker 模型和直接在主线程执行任务的 worker 模型。基于 fork 的 worker 在执行任务之前先 fork 一个子进程，在子进程中执行具体的任务，父进程等待子进程执行返回。在基于 fork 的 worker 模型下，如果在父进程有一个 MySQL/redis 连接，由于子进程会继承父进程的地址空间，具有相同的打开文件、socket、管道等，所以子进程中也有同样的 MySQL/redis 连接，那么在这种情况下这个连接可以直接使用么？通过以下代码简单测试一下，连接 MySQL 使用 torndb ，连接 redis 使用 redis-py：","text":"最近使用了 rq 这个简单的队列处理库，其中有一些任务需要使用MySQL的连接或者redis的连接，对此有一些思考。 MySQL/redis的连接复用rq 提供了两种 worker 模型：基于 fork 的 worker 模型和直接在主线程执行任务的 worker 模型。基于 fork 的 worker 在执行任务之前先 fork 一个子进程，在子进程中执行具体的任务，父进程等待子进程执行返回。在基于 fork 的 worker 模型下，如果在父进程有一个 MySQL/redis 连接，由于子进程会继承父进程的地址空间，具有相同的打开文件、socket、管道等，所以子进程中也有同样的 MySQL/redis 连接，那么在这种情况下这个连接可以直接使用么？通过以下代码简单测试一下，连接 MySQL 使用 torndb ，连接 redis 使用 redis-py： 123456789101112131415161718192021222324252627#!/usr/bin/env python# encoding: utf-8import redisimport torndbdef init_redis_conn(): return redis.StrictRedis(port=6380)def init_mysql_conn(): return torndb.Connection( host=&#x27;127.0.0.1&#x27;, user=&#x27;root&#x27;, password=&#x27;password&#x27;, database=&#x27;test&#x27;)_redis_conn = init_redis_conn()_mysql_conn = init_mysql_conn()def get_redis_conn(): global _redis_conn if _redis_conn is None: _redis_conn = init_redis_conn() return _redis_conndef get_mysql_conn(): global _mysql_conn if _mysql_conn is None: _mysql_conn = init_mysql_conn() return _mysql_conn 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# encoding: utf-8import osimport timeimport errnoimport mconndef redis_op(): conn = mconn.get_redis_conn() k = int(time.time()) conn.set(k, k)def mysql_op(): conn = mconn.get_mysql_conn() sql = &#x27;INSERT INTO users (name) values (%s)&#x27; conn.insert(sql, str(int(time.time())))def fork_test(func): child_pid = os.fork() if child_pid == 0: func() os._exit(0) else: try: os.waitpid(child_pid, 0) except OSError as e: if e.errno != errno.EINTR: raiseif __name__ == &#x27;__main__&#x27;: while True: time.sleep(0.5) # fork_test(mysql_op) fork_test(redis_op) 执行 main.py，观察3306端口和6380端口的连接数，发现 MySQL 的连接会被复用，但是在 redis 的连接并没有复用，产生了大量到 redis 的 TCP 连接。查看一下 redis-py 的代码，很容易发现为什么每个工作子进程都新建了一个到 redis 的连接。redis-py 通过 StrictRedis 对象向 redis 发起命令时，首先调用 ConnectionPool 对象的 get_connection 方法获取一个可用的连接。在 get_connection 方法中，会首先调用 _checkpid 函数。_checkpid 检查 connnection_pool 的 pid与当前进程的pid是否一致，如果不相同，会关闭 connection_pool 中的所有连接，然后重新建立到 redis 的 TCP 连接。相关代码如下所示： 1234567891011121314151617181920212223class StrictRedis(object): def execute_command(self, *args, **options): &quot;Execute a command and return a parsed response&quot; pool = self.connection_pool connection = pool.get_connection(command_name, **options) # connection send_command ...class ConnectionPool(object): def get_connection(self, command_name, *keys, **options): &quot;Get a connection from the pool&quot; self._checkpid() # get connection ... return connection def _checkpid(self): if self.pid != os.getpid(): with self._check_lock: if self.pid == os.getpid(): # another thread already did the work while we waited # on the lock. return self.disconnect() self.reset() 通过上述代码就很清楚为什么在 fork 工作模式下 redis 连接没有复用了，ConnectPool 对象的 pid 是父进程的 pid，在子进程中与子进程的 pid不同，于是到 redis 的连接被重置。 fork 模型下共享 MySQL 连接合理么不合理，会有很多问题。比如在子进程中到 MySQL 的 TCP 连接因为异常关闭（或者主动调用 db.close()），由于 copy-on-write 的特性，子进程中修改 *db = None (*db 是 torndb 中保存到 MySQL连接的对象)并不会影响父进程的值，父进程再次 fork 出的子进程使用该 torndb 连接对象时就会出现 OperationalError: (OperationalError) (2006, &#39;MySQL server has gone away&#39;) 的错误。使用 uWSGI 就会有这样的使用场景，于是 uWSGI 有了 lazy-apps 的选项。 为什么 rq 提供了基于 fork 的 worker 模型回到最初的问题，现在需要在 rq 的任务中使用 MySQL 或者 redis 连接，如果使用 fork 模型的 worker，就需要每次重建一个 TCP 连接，这会带来很大的性能开销，通常是不可接受的。直接使用另外一种在主线程执行任务的 worker 似乎是更好的方案。那么为什么 rq 提供了基于 fork 的 worker 模型？rq 的作者给出了这样的理由： This mainly has to do with stability. When you spawn a child process (with fork(), or multiprocessing, or whatever) you get an isolated execution context, which has a few nice benefits. Some of which are: 1. If a process crashes (by a segfault in a C module for example), only the child crashes; 2. Additionally, the worker will always be responsive and can easily kill the child after a time out; 3. Also, memory leaks caused in the child can never affect the main worker. The child is killed after every job, so memory should never grow, even when running rqworker for long periods of time. rq 所看到的任务是一个可加载的 python 函数对象，执行任务时加载该对象并传入参数执行，对于可能出现的任务执行崩溃或内存泄漏等情况 rq 本身并不能处理（比如提到的 C 模块段错误，python 的 try-except 是无法捕捉的）。作为一个执行任务的通用库，fork-based worker 采用了一种保守的手段，通过进程级别的隔离保证了主进程的稳定运行。 关于 rq 的使用场景最后，谈一谈 rq 在不同场景下的使用。 当有很多小任务、每个任务可能需要等待IO，这种情况下使用非阻塞模型最适合了，比如 gevent。那么 rq 是否支持 gevent？目前是没有官方支持的，当然有一些第三方的扩展，需要注意的是使用 gevent 时最好是重写worker执行的入口，即 rqworker，因为如果只是在 -w 对应的 worker 类中使用 gevent，在 monkey patch 之前已经引用了一些模块，可能会有未知的问题。一些实现参考：rq-gevent-worker, gevent_rqworker.py 另外一种场景，执行的任务是 CPU 密集型的，通常使用多进程比较合适。rq 对于支持使用多进程并发执行任务的 worker 也没有官方支持，一种解决方案是启动多个 rqorker 进程来从同一个任务队列消费任务；当然，也可以自己去扩展 rqworker。 总体而言，rq 代码本身实现得比较简洁，只支持 redis 作为队列存储任务，比较适合一些轻量级的异步任务处理。另一方面由于是一个通用库，一些具体场景下的需求就需要使用者自己来定制。","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"redis","slug":"redis","permalink":"http://amyangfei.me/tags/redis/"},{"name":"job queue","slug":"job-queue","permalink":"http://amyangfei.me/tags/job-queue/"}]},{"title":"SSH through different kinds of proxy","slug":"ssh-and-proxy","date":"2015-01-24T00:00:00.000Z","updated":"2020-12-13T07:05:03.613Z","comments":true,"path":"2015/01/24/ssh-and-proxy/","link":"","permalink":"http://amyangfei.me/2015/01/24/ssh-and-proxy/","excerpt":"有时候因为网络、安全等原因，我们不能通过 ssh 直接连接到目标主机，而是需要通过代理服务器或跳板机实现连接。本文总结通过代理或跳板机使用 ssh 的各种方法，并且分析这些方法的基本原理。 我们设定本地主机的地址为 homepc，绑定有公网 ip；运行有各类代理的代理服务器或跳板机地址为 proxy-server，proxy-server 上绑定一个公网 ip，同时绑定一个内网 ip（假定为10.0.10.252）；需要连接的目标主机 target-server，绑定内网 ip（假定为 10.0.10.25）。所有的用户名、登录用户名使用 apple。","text":"有时候因为网络、安全等原因，我们不能通过 ssh 直接连接到目标主机，而是需要通过代理服务器或跳板机实现连接。本文总结通过代理或跳板机使用 ssh 的各种方法，并且分析这些方法的基本原理。 我们设定本地主机的地址为 homepc，绑定有公网 ip；运行有各类代理的代理服务器或跳板机地址为 proxy-server，proxy-server 上绑定一个公网 ip，同时绑定一个内网 ip（假定为10.0.10.252）；需要连接的目标主机 target-server，绑定内网 ip（假定为 10.0.10.25）。所有的用户名、登录用户名使用 apple。 首先我们介绍一些常见的连接方法 登录跳板机，在跳板机上连接目标主机 方法A：直接登录，可以通过 -A 选项利用Agent forwarding 特性 1234567# now on homepcapple@homepc➜ ssh -A apple@proxy-server# now on proxy-serverapple@proxy-server➜ ssh apple@10.0.10.25 方法B：A useful trick，通过 -tt 强制分配 tty，直接执行 ssh 命令 12apple@homepc➜ ssh -A -tt apple@proxy-server ssh apple@10.0.10.25 方法C：利用 netcat 在跳板机上建立 tunnel，通过此 tunnel 连接目标主机 12apple@homepc➜ ssh -oProxyCommand=&#x27;ssh apple@proxy-server nc %h %p&#x27; apple@10.0.10.25 借助 proxy 连接目标主机 方法D：本地 ssh 代理，在 proxy-server 上用户 apple 设有 nologin 的 shell 权限，不能通过 ssh 登录 proxy-server 但是可以进行 ssh 连接，通过 -D 进行本地的端口转发，详情可以查看 man ssh。 123456# run ssh daemon on homepc for local “dynamic” application-level port forwardingapple@homepc➜ ssh -N -D 12171 apple@proxy-server &amp;apple@homepc➜ ssh -oProxyCommand=&#x27;nc -x 127.0.0.1:12171 %h %p&#x27; apple@10.0.10.25 方法E：类似于本地 ssh 代理的方式，可以在 proxy-server 上运行任何协议类型的代理，在 homepc 本地运行代理客户端连接 proxy-server 上的 proxy，在 ssh 的 ProxyCommand 指定为本地代理客户端的连接点即可。 方法F：利用 corkscrew ，connect-proxy，proxychains 等直接连接 proxy-server 上的代理。假定 proxy-server 上运行有 squid http 代理，代理使用用户名/密码这种基本验证方式。corkscrew 仅支持 http 代理，connect-proxy 和 proxychains 支持 http, socks4, socks5 代理。proxychains 还支持 shell 内所有流量都通过代理，提供了更过的功能，这里不展开叙述。只简单看一下通过它们使用 http 代理连接 ssh 的情况。 1234567891011121314# corkscrew can specify authfile with pattern of username:password for http proxy authenticationapple@homepc➜ ssh -oProxyCommand=&#x27;corkscrew proxy-server 3128 %h %p ~/.ssh/authfile&#x27; apple@10.0.10.25# use connect-proxyapple@homepc➜ ssh -oProxyCommand=&#x27;connect -H apple@proxy-server:3128 %h %p&#x27; apple@10.0.10.25# use proxychains# add &quot;http xxx.yyy.zzz.www 3128 apple apple&quot; to &quot;[ProxyList]&quot; node in proxychains config file# xxx.yyy.zzz.www is the WAN ip of proxy-server. proxychains doesn&#x27;t support dns lookup for proxy server# more details: https://github.com/rofl0r/proxychains-ng/issues/25apple@homepc➜ proxychains4 ssh apple@10.0.10.25 ProxyCommand 关于 ProxyCommand 在此不多叙述，详情参考 man ssh_config。可以在 ~/.ssh/config 中配置 ProxyCommand，还可以根据不同的 host 配置不同的 ProxyCommand。 原理分析 这些方法看起来有些眼花缭乱，但其实原理都很简单，除去登录到跳板机的情形，剩下场景的都是通过某种形式连接到代理服务器上的代理（或通过更多层的代理连接到代理服务器上的代理，形成一个代理链），由代理转发数据到目标服务器。 首先看方法C 的场景，参考transparent-proxy-with-ssh +--------+ +--------------+ +---------------+ | homepc | | proxy-server | | target-server | | | ===ssh=over=netcat=tunnel======================&gt; | 10.0.10.25 | +--------+ +--------------+ +---------------+ 该场景中，proxy-server 上实际运行有 nc 10.0.10.25 22 进程，该进程将会完成数据在 homepc 和 target-server 之间的转发。 方法D, E, F 中包含有明显的代理，以方法F 中的 corkscrew + squid http proxy 进行分析。 ssh 指定使用 ProxyCommand 之后，在建立连接时有这样一段关键代码： 123456789101112// from openssh-5.9p1, sshconnect.cintssh_connect(const char *host, struct sockaddr_storage * hostaddr, u_short port, int family, int connection_attempts, int *timeout_ms, int want_keepalive, int needpriv, const char *proxy_command)&#123; ... /* If a proxy command is given, connect using it. */ if (proxy_command != NULL) return ssh_proxy_connect(host, port, proxy_command); ...&#125; 在 ssh_proxy_connect 中会 fork 出子进程来执行 ProxyCommand 中的命令，同时会重定向子进程的标准输入和标准输出，子进程的标准输入重定向到 pin[0]，所以子进程会通过 pin[1] 获得父进程标准输出的内容；子进程的标准输出重定向到 pout[1]，所以写到子进程标准输出的内容可以在父进程通过读取 pout[0] 获得。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// from openssh-5.9p1, sshconnect.cstatic intssh_proxy_connect(const char *host, u_short port, const char *proxy_command)&#123; ... /* Fork and execute the proxy command. */ if ((pid = fork()) == 0) &#123; ... /* Redirect stdin and stdout. */ close(pin[1]); if (pin[0] != 0) &#123; if (dup2(pin[0], 0) &lt; 0) perror(&quot;dup2 stdin&quot;); close(pin[0]); &#125; close(pout[0]); if (dup2(pout[1], 1) &lt; 0) perror(&quot;dup2 stdout&quot;); /* Cannot be 1 because pin allocated two descriptors. */ close(pout[1]); /* Stderr is left as it is so that error messages get printed on the user&#x27;s terminal. */ argv[0] = shell; argv[1] = &quot;-c&quot;; argv[2] = command_string; argv[3] = NULL; /* Execute the proxy command. Note that we gave up any extra privileges above. */ signal(SIGPIPE, SIG_DFL); execv(argv[0], argv); perror(argv[0]); exit(1); &#125; ... /* Close child side of the descriptors. */ close(pin[0]); close(pout[1]); /* Set the connection file descriptors. */ packet_set_connection(pout[0], pin[1]); ...&#125; +----------+ +---------------+ +----------+ +---------------+ | terminal | --------&gt; | parent stdout | ----------&gt; | pin[0] | ---------&gt; | child stdin | | input | | pin[1] | read | | redirect | | +----------+ +---------------+ +----------+ +---------------+ +----------+ +---------------+ +----------+ +---------------+ | terminal | &lt;-------- | parent stdin | &lt;---------- | pout[1] | &lt;--------- | child stdout | | display | | pout[0] | read | | redirect | | +----------+ +---------------+ +----------+ +---------------+ 上图描述了调用 ProxyCommand 时 ssh 客户端数据的流动情况，在我们的应用场景中，父进程对应 ssh 客户端进程，子进程运行 corkscrew。corkscrew 的实现很简单，它与代理服务器创建 tcp 连接，然后进入一个主循环，通过 select(2) 处理文件事件。（注意 corkscrew 将与代理服务器协商的代码也写在主循环中，通过 setup 标志来确定处于建立连接后的协商阶段还是已经建立好到代理的连接，协商部分的代码可以抽离出来，类似的 connect-proxy 就是抽离出协商阶段和稳定连接阶段。下边分析的主循环略过协商阶段的代码。），这样处理文件读写事件的代码就非常简单： 123456789101112131415161718192021222324252627// from corkscrew2.0, corkscrew.cfor(;;) &#123; FD_ZERO(&amp;sfd); FD_ZERO(&amp;rfd); FD_SET(csock, &amp;rfd); FD_SET(0, &amp;rfd); tv.tv_sec = 5; tv.tv_usec = 0; if(select(csock+1,&amp;rfd,&amp;sfd,NULL,&amp;tv) == -1) break; if (FD_ISSET(csock, &amp;rfd)) &#123; len = read(csock, buffer, sizeof(buffer)); if (len&lt;=0) break; len = write(1, buffer, len); if (len&lt;=0) break; &#125; if (FD_ISSET(0, &amp;rfd)) &#123; len = read(0, buffer, sizeof(buffer)); if (len&lt;=0) break; len = write(csock, buffer, len); if (len&lt;=0) break; &#125;&#125; corkscrew 的处理逻辑很清楚，从标准输入读取的数据，write 到 csock 中；从 csock 读取的数据，write 到标准输出。与 ssh 客户端结合起来，就可以得到下边的一张图： +--------------+ +------------+ +-----------------+ +---------+ | child stdin | --------&gt; | corkscrew | ----------&gt; | csock | -----&gt; | proxy | | corkscrew | read | | write | conn with proxy | | | +--------------+ +------------+ +-----------------+ +---------+ +--------------+ +------------+ +-----------------+ +---------+ | child stdout | &lt;-------- | corkscrew | &lt;---------- | csock | &lt;----- | proxy | | corkscrew | write | | read | conn with proxy | | | +--------------+ +------------+ +-----------------+ +---------+ 从代理到目标服务器的数据收发与上述实现类似，只是 socket 有所不同，不再按照不同代理具体分析。使用不同的代理形式，只是在代理协商阶段有所不同，当稳定连接后，代理的工作就是不停的转发数据了。从数据的发送接收角度，加入代理后不影响 ssh 客户端和服务器之间传输数据的内容和顺序，因而可以将代理看做是透明的，就好像 ssh 客户端直接连接到目标服务器一样。 问答TODO 使用代理会不会存在安全问题？例如 Security Issues With Key Agents 提到的安全问题。 我使用 mosh，这些方法是否可以使用？ 如果我使用公私钥登录的方式，代理服务器上需要进行哪些操作？ 参考链接 Using SSH ProxyCommand to Tunnel Connections SSH Through or Over Proxy Corkscrew OpenSSH/Cookbook/Proxies and Jump Hosts An Illustrated Guide to SSH Agent Forwarding SSH through HTTP proxy 透过代理连接SSH","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"http://amyangfei.me/tags/ssh/"},{"name":"proxy","slug":"proxy","permalink":"http://amyangfei.me/tags/proxy/"}]},{"title":"python 拾遗2","slug":"python-tips-2","date":"2014-12-04T00:00:00.000Z","updated":"2020-12-13T09:08:33.155Z","comments":true,"path":"2014/12/04/python-tips-2/","link":"","permalink":"http://amyangfei.me/2014/12/04/python-tips-2/","excerpt":"本文是上一篇文章 python 拾遗 的延续，继续整理 python 的一些使用技巧，以及一些可能被忽略的细节 注意: 以下讨论主要为 Python2.7 版本， Python 3 的内容有待跟进","text":"本文是上一篇文章 python 拾遗 的延续，继续整理 python 的一些使用技巧，以及一些可能被忽略的细节 注意: 以下讨论主要为 Python2.7 版本， Python 3 的内容有待跟进 Get MD5 hash of big files当我们需要通过 python 得到一个很大文件的 md5 值的时候，我们可以通过分段读取文件的方法来节约内存，选择合适的分段大小还会适当提高计算效率。chksum.py 通过 memory_profiler 统计执行过程中内存的使用情况并统计每一次计算的执行时间，同时给出了1Gb 数据的测试结果。 stackoverflow 上的一些讨论：Get MD5 hash of big files in Python, Lazy Method for Reading Big File in Python io.BytesIO vs cString.StringIOpython2 和 python3 在 StringIO 和 BytesIO 之间有诸多不同，six 是一个提供同时兼容 py2 和 py3 的解决方案，这个几个模块的具体区别参考下边的表格。 模块 Python 2 Python 3 StringIO.StringIO 内存中的字符串缓存，可以存储字符串或Unicode 类型 删除 cStringIO.StringIO 基于C实现提供类似StringIO.StringIO的接口且更高效，但是相比StringIO.StringIO使用有一定限制 删除 io.StringIO 对 Unicode文本内容的内存缓存，只能存储 Unicode 对象 对文本数据的内存缓存，不能接收 Unicode 类型 io.BytesIO 存储字节的内存缓存 存储字节的内存缓存 six.StringIO StringIO.StringIO io.StringIO six.BytesIO StringIO.StringIO io.BytesIO 在性能上：通常 cStringIO.StringIO 是最快的。io.Bytes 同样是通过 C 实现的，但是例如通过 io.BytesIO(b'data') 初始化 BytesIO 对象时会对数据进行一次复制，这会引起性能上的损失。 关于 StringIO 和 BytesIO 的性能区别，对于 IO 性能敏感的场景还是有很大影响，例如在 tornado，scrapy 的项目中以及 Python 邮件列表 中都有相关讨论。在未来 Python3.5 版本中将会对 io.BytesIO 进行 copy-on-write 的优化，详见：Python Issue22003。 当具体需要创建 file-like 的数据流时并且需要考虑对 Python2 和 Python3 代码的兼容性时，我们需要根据具体的数据类型（字符串或者 Unicode 或者 Bytes），以及使用场景对性能的要求选择合适的模块。 List comprehensions leak the loop control variable看一段很简单的列表生成的代码： 12345678&gt;&gt;&gt; x = &#x27;before&#x27;&gt;&gt;&gt; a = [x for x in range(5)]&gt;&gt;&gt; x4&gt;&gt;&gt; x = &#x27;before&#x27;&gt;&gt;&gt; a = (x for x in range(5))&gt;&gt;&gt; x&#x27;before&#x27; 在 python2.x 中，list comprehension 中变量的作用域并不仅限于 [] 中，而是会泄露出来，而 Generator expressions 执行时会创建一个独立的运行域，因而不会发生变量泄露。在 Python3 中 list comprehension 变量泄露已经得到了修改。 下边是 Python History 中的原文 This was an artifact of the original implementation of list comprehensions; it was one of Python’s “dirty little secrets” for years. It started out as an intentional compromise to make list comprehensions blindingly fast, and while it was not a common pitfall for beginners, it definitely stung people occasionally. For generator expressions we could not do this. Generator expressions are implemented using generators, whose execution requires a separate execution frame. Thus, generator expressions (especially if they iterate over a short sequence) were less efficient than list comprehensions. socket.settimeout(value)socket 设置超时之后，该 socket 就是 non-blocking 模式 Timeout mode internally sets the socket in non-blocking mode. The blocking and timeout modes are shared between file descriptors and socket objects that refer to the same network endpoint. A consequence of this is that file objects returned by the makefile() method must only be used when the socket is in blocking mode; in timeout or non-blocking mode file operations that cannot be completed immediately will fail.","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"}]},{"title":"python 拾遗","slug":"python-tips","date":"2014-11-27T00:00:00.000Z","updated":"2020-12-13T07:04:49.339Z","comments":true,"path":"2014/11/27/python-tips/","link":"","permalink":"http://amyangfei.me/2014/11/27/python-tips/","excerpt":"整理 python 使用的一些技巧，以及一些可能被忽略的细节，很多在文档可以查找到的内容将不会过多的描述，更多以外链的形式存在。 注意: 以下讨论主要为 Python2.7 版本， Python 3 的内容有待跟进","text":"整理 python 使用的一些技巧，以及一些可能被忽略的细节，很多在文档可以查找到的内容将不会过多的描述，更多以外链的形式存在。 注意: 以下讨论主要为 Python2.7 版本， Python 3 的内容有待跟进 importpython 的 import 通过调用 __import__(name[, globals[, locals[, fromlist[, level]]]]) 这个函数实现，借助这个函数可以通过 python 模块的名字动态引用模块。来自tornado 的 import_object 是一个很简洁的封装。 1234567891011121314def import_object(name): &quot;&quot;&quot;Imports an object by name. import_object(&#x27;x&#x27;) is equivalent to &#x27;import x&#x27;. import_object(&#x27;x.y.z&#x27;) is equivalent to &#x27;from x.y import z&#x27;. &quot;&quot;&quot; if name.count(&#x27;.&#x27;) == 0: return __import__(name, None, None) parts = name.split(&#x27;.&#x27;) obj = __import__(&#x27;.&#x27;.join(parts[:-1]), None, None, [parts[-1]], 0) try: return getattr(obj, parts[-1]) except AttributeError: raise ImportError(&quot;No module named %s&quot; % parts[-1]) 除此之外，我们还可以利用 python2.7 开始提供的 importlib.import_module （实现代码）来进行动态引用。例如：importlib.import_module('tornado.httpclient') reload当 reload 一个 python 模块之后有两处不会使用新的模块的值： 原来已经使用的实例还是会使用旧的模块，而新生产的实例会使用新的模块； 其他模块引用该模块的对象，这些引用不会绑定到新的对象上。 python-reloader 是一个很有趣的项目，它将 __builtin__.__import__ 修改为自定义的 _import 函数，新的 _import 在原有调用 __import__(name[, globals[, locals[, fromlist[, level]]]]) 的同时记录下引用模块之间的依赖关系。python-reloader 实现的是 reload 一个模块之后，reload 该模块所依赖的所有模块，而不是 reload 所有依赖该模块的模块。说起来很绕，这里 issue(It should reload dependants instead of dependencies)有很好的讨论。在实际运行的系统中动态 reload 模块可能并非一种很好的选择。 Catching an exception while using ‘with’当我们使用 with 表达式并且需要捕捉异常的时候，我们可以这样做： 12345try: with open( &quot;foo.txt&quot; ) as f : print f.readlines()except EnvironmentError: # parent of IOError, OSError print &#x27;oops&#x27; 如果希望捕捉 with 表达式的异常与内部工作代码的异常分离出来，我们可以这样做： 1234567try: f = open(&#x27;foo.txt&#x27;)except IOError: print &#x27;oops&#x27;else: with f: print f.readlines() 关于 python 使用 try-except-else, with 的一些讨论：Is it a good practice to use try-except-else，Using python “with” statement with try-except block StringIO当我们使用一些接收参数是文件类型的 API 时，我们可能需要使用到 StringIO，例如使用 gzip 模块压缩一个字符串： 12345678import gzip, StringIOstringio = StringIO.StringIO()gzip_file = gzip.GzipFile(fileobj=stringio, mode=&#x27;w&#x27;)gzip_file.write(&#x27;Hello World&#x27;)gzip_file.close()stringio.getvalue() 在 Python2.7 中 cStringIO 提供了与 StringIO 类似的接口，并且运行效率更高。在 Python3.4 中这两者被统一成为了 io.StringIO。 注意：cStringIO 的使用有一些限制：cStringIO 不能作为基类被继承；cStringIO 不能接收非 ASCII 字符的字符串参数；还有一点与 StringIO 不同的是当使用字符串参数初始化一个 cStringIO 对象时，该对象是只读的。 Queue.Queue vs collections.dequeQueue (python3 重命名为 queue)是一个可用于多线程之间同步、交换数据的队列模块，包括 FIFO，LIFO，优先级队列三个实现。 collections.deque 是一个双端队列的数据结构，在头和尾的插入、删除、读取操作是O(1)复杂度；在队列中部的随机读取操作是O(n)的。 reference：Python: Queue.Queue vs. collections.deque heapqpython 内置的 heapq 是一个小顶堆，并且 heapify, heappush, heappop 操作是不支持传递 key 参数的。如果想实现大顶堆，可以这样 lambda x: -x，或者自己封装一层。例如：python topN max heap。另外，heapq.nlargest 和 heapq.nsmallest 支持 key 参数。邮件列表里的讨论：为什么 python 的 heapq 没有支持 key 参数 itertools.teeitertools.tee 从一个迭代器返回 n 个独立的迭代器，原始迭代器将不允许被使用，如果使用，那么可能会导致新的迭代器失效。Inside Python’s itertools.tee 很详细的探究了 itertools.tee 的实现细节。 when should we use operator最常用的就是 operator.itemgetter，例如我们有一个 tuple 列表，需要对这些元组按照第i个元素排序，那么可以这样：lst.sort(key=operator.itemgetter(i))。 operator.add 与 lambda x, y: x+y 具有相同的效果，二者的不同主要有两个方面：一方面是它们的可读性、开发者的使用习惯的差别；另一方面是性能差别。在python wiki 中有这样一段话： Likewise, the builtin functions run faster than hand-built equivalents. For example, map(operator.add, v1, v2) is faster than map(lambda x,y: x+y, v1, v2). 我们对二者做一次简单的实验对比： 1234567891011➜ python -m timeit &#x27;import operator&#x27; &#x27;map(operator.add, [x for x in range(5000)], [y for y in range(5000)])&#x27;1000 loops, best of 3: 710 usec per loop➜ python3 -m timeit &#x27;import operator&#x27; &#x27;map(operator.add, [x for x in range(5000)], [y for y in range(5000)])&#x27;1000 loops, best of 3: 401 usec per loop➜ python -m timeit &#x27;map(lambda x,y:x+y, [x for x in range(5000)], [y for y in range(5000)])&#x27;100 loops, best of 3: 929 usec per loop➜ python3 -m timeit &#x27;map(lambda x,y:x+y, [x for x in range(5000)], [y for y in range(5000)])&#x27;1000 loops, best of 3: 397 usec per loop 实验结果中，在 Python2.7 环境下 operator.add 稍微快于使用 lambda 表达式，在 Python3 环境下两者几乎没有差别。事实上 Python 并不适合 CPU 密集型的应用场景，当 CPU 不是性能瓶颈时，operator 和 lambda 之间的性能差距基本可以忽略。","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"}]},{"title":"Making Ebooks of Pro Git","slug":"making-progit-ebook","date":"2014-01-10T00:00:00.000Z","updated":"2020-12-13T07:04:40.695Z","comments":true,"path":"2014/01/10/making-progit-ebook/","link":"","permalink":"http://amyangfei.me/2014/01/10/making-progit-ebook/","excerpt":"Pro Git 是一本用来学习 git 使用很不错的书，从 progit 这个开源项目可以获得这本书的全部内容，下边介绍在 Ubuntu Server 12.04 环境下制作电子书的过程。 安装依赖1. ruby, rubygems参考 Ruby-China 的 wiki 2. rdiscountmarkdown 使用的模板，通过 rubygems 安装 123$ gem install rdiscount 3. calibrecalibre 是一款开源的电子书管理软件，生成 epub 或 mobi 格式需要安装。 123$ sudo apt-get install calibre","text":"Pro Git 是一本用来学习 git 使用很不错的书，从 progit 这个开源项目可以获得这本书的全部内容，下边介绍在 Ubuntu Server 12.04 环境下制作电子书的过程。 安装依赖1. ruby, rubygems参考 Ruby-China 的 wiki 2. rdiscountmarkdown 使用的模板，通过 rubygems 安装 123$ gem install rdiscount 3. calibrecalibre 是一款开源的电子书管理软件，生成 epub 或 mobi 格式需要安装。 123$ sudo apt-get install calibre 4. pandoc，xelatex生成 pdf 格式需要安装这两个依赖，pandoc 安装比较简单，直接apt；安装 xelatex 则需要首先安装 texlive-xetex，然后打上 LaTeX::Driver 补丁（参考了这个网址 Frequently Asked Questions - XeLaTeX ）。 12345$ sudo apt-get install pandoc$ sudo apt-get install texlive-xetex texlive-latex-base texlive-latex-extra$ sudo apt-get install liblatex-&#123;driver,encode,table&#125;-perl 5. install font生成 pdf 时读取 latex/config.yml 中的配置，使用过程中可能会出现一些字体不存在的错误。比如： 12345678./makepdf enen: Parsing markdown... done Creating main.tex for en... done Running XeTeX: Pass 1... failed with: ! I can&#x27;t find file `Helvetica&#x27;. Consider running this again with --debug. 这个错误的原因是 Ubuntu 系统中 Helvetica 字体名字为 Nimbus Sans L，修改 confi.yml 文件default 节点下的 font 值即可。 1234$ fc-match &quot;Helvetica Neue&quot;DejaVuSans.ttf: &quot;DejaVu Sans&quot; &quot;Book&quot;$ fc-match &quot;Helvetica&quot;n019003l.pfb: &quot;Nimbus Sans L&quot; &quot;Regular&quot; 生成中文pdf时同样遇到了中文字体不存在的情况，默认的配置文件使用的是 AR PL UMing CN 和 AR PL UKai CN，直接安装即可。但是我个人感觉 AR PL UMing CN 字体太细瘦，换成了文泉驿微米黑字体，圆润多了。 123sudo apt-get install ttf-arphic-ukai # &quot;AR PL UKai&quot; 文鼎PL中楷sudo apt-get install ttf-arphic-uming # &quot;AR PL UMing&quot; 文鼎PL细上海宋sudo apt-get install ttf-wqy-microhei # &quot;WenQuanYi Micro Hei&quot; 文泉驿微米黑 make最后一步，生成电子书，好书一本，细细品读，Enjoy！ 1234$ ./makeebooks en # 默认生成mobi格式，en英文$ export FORMAT=epub # 设置FORMAT为epub，生成epub格式$ ./makeebooks zh # 生成epub格式，zh中文$ ./makepdfs zh # 生成pdf","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[]},{"title":"APScheduler 源码阅读笔记","slug":"apscheduler-source-analyse","date":"2013-11-06T00:00:00.000Z","updated":"2020-12-20T09:09:39.162Z","comments":true,"path":"2013/11/06/apscheduler-source-analyse/","link":"","permalink":"http://amyangfei.me/2013/11/06/apscheduler-source-analyse/","excerpt":"概述APScheduler 是由 python 实现的一个轻量级任务调度器，它可以按照一定间隔（IntervalTrigger）、指定时间（2.1中的SimpleTrigger/3.0中的DateTrigger）或者以类似 cron（CronTrigger） 的形式触发待执行任务（即调用函数或者调用 python 的 callable 对象）。现在 pypi 上的稳定版是 APScheduler 2.1.1，3.0 版本在 class Scheduler 中移除了针对不同 trigger 的 add_trigger_job() 接口，统一为 add_job()，但是底层实现变化不大。我主要看了 2.1.1 的代码。代码很简洁，加起来一共2049行。 模块组织 Scheduler 调度器的核心部分，负责对 job 的管理和调度，用户使用添加/移除任务，启动调度器都通过 Scheduler 提供的接口完成。 Job 封装了需要调度的任务，每一个 Job 实例是在 Scheduler 添加 job 时被初始化，具体的初始化参数决定了调度被触发的形式（3类不同的trigger）。 Trigger 包含 SimpleTrigger，IntervalTrigger和 CronTrigger 三个类。Trigger 的作用就是计算下一次触发任务的时间。 JobStore 抽象基类，针对任务存储的介质有多个实现，包括基于内存（RAMJobStore）、使用shelve的简单持久化存储（ShelveJobStore）、使用数据库存储（RedisJobStore，MongoDBJobStore）等。如果不指定参数默认使用 RAMJobStore，使用持久化的 JobStore 的目的是在 Scheduler 重启之后能够恢复原有的任务调度。 底层实现从分析 Scheduler 类入手，首先看项目中自带的example：","text":"概述APScheduler 是由 python 实现的一个轻量级任务调度器，它可以按照一定间隔（IntervalTrigger）、指定时间（2.1中的SimpleTrigger/3.0中的DateTrigger）或者以类似 cron（CronTrigger） 的形式触发待执行任务（即调用函数或者调用 python 的 callable 对象）。现在 pypi 上的稳定版是 APScheduler 2.1.1，3.0 版本在 class Scheduler 中移除了针对不同 trigger 的 add_trigger_job() 接口，统一为 add_job()，但是底层实现变化不大。我主要看了 2.1.1 的代码。代码很简洁，加起来一共2049行。 模块组织 Scheduler 调度器的核心部分，负责对 job 的管理和调度，用户使用添加/移除任务，启动调度器都通过 Scheduler 提供的接口完成。 Job 封装了需要调度的任务，每一个 Job 实例是在 Scheduler 添加 job 时被初始化，具体的初始化参数决定了调度被触发的形式（3类不同的trigger）。 Trigger 包含 SimpleTrigger，IntervalTrigger和 CronTrigger 三个类。Trigger 的作用就是计算下一次触发任务的时间。 JobStore 抽象基类，针对任务存储的介质有多个实现，包括基于内存（RAMJobStore）、使用shelve的简单持久化存储（ShelveJobStore）、使用数据库存储（RedisJobStore，MongoDBJobStore）等。如果不指定参数默认使用 RAMJobStore，使用持久化的 JobStore 的目的是在 Scheduler 重启之后能够恢复原有的任务调度。 底层实现从分析 Scheduler 类入手，首先看项目中自带的example： 123456789101112131415161718from datetime import datetimefrom apscheduler.scheduler import Schedulerdef tick(): print(&#x27;Tick! The time is: %s&#x27; % datetime.now())if __name__ == &#x27;__main__&#x27;: scheduler = Scheduler(standalone=True) scheduler.add_interval_job(tick, seconds=3) print(&#x27;Press Ctrl+C to exit&#x27;) try: scheduler.start() except (KeyboardInterrupt, SystemExit): pass 上边代码的最核心的三行就是初始化Scheduler，添加以interval为触发的 job 和启动scheduler。这也是使用APScheduler 最基本也最主要的方式。 初始化 Scheduler 有很多参数可以选择（详细可以参考 scheduler-configuration-options），这里简单介绍 standalone 和 daemonic 两个参数。standalone 设置为 False，那么 scheduler 将会以 embedded 模式运行，该模式下调度器会在一个新的线程中运行调度循环(_main_loop)；如果 standlone 设置为True，那么 scheduler 会阻塞当前线程，执行调度循环，直到不再有调度任务后返回，被阻塞的线程继续运行。daemonic 即是否以守护线程运行 scheduler，与python 守护线程的效果一致，如果 daemonic 设置为 False，显然该参数在 embedded 模式（standalone==False）下才有效果。Scheduler 默认的运行参数是 standalone == False, daemonic == True，即以 embedded 模式的守护线程中运行调度循环。 start 是启动 scheduler 的方法，如下所示。代码很简洁，启动前读取所有 job_store 中pending job（pending job 是 scheduler 未启动前添加的job），如果为 standalone 模式，会直接进入 _main_loop 调度循环，否则在新的线程中运行调度循环。 1234567891011121314151617181920def start(self): if self.running: raise SchedulerAlreadyRunningError # Create a RAMJobStore as the default if there is no default job store if not &#x27;default&#x27; in self._jobstores: self.add_jobstore(RAMJobStore(), &#x27;default&#x27;, True) # Schedule all pending jobs for job, jobstore in self._pending_jobs: self._real_add_job(job, jobstore, False) del self._pending_jobs[:] self._stopped = False if self.standalone: self._main_loop() else: self._thread = Thread(target=self._main_loop, name=&#x27;APScheduler&#x27;) self._thread.setDaemon(self.daemonic) self._thread.start() _main_loop 就是调度循环，主体就是一个 while 循环。 12345678910111213141516while not self._stopped: logger.debug(&#x27;Looking for jobs to run&#x27;) now = datetime.now() next_wakeup_time = self._process_jobs(now) # Sleep until the next job is scheduled to be run, a new job is added or the scheduler is stopped if next_wakeup_time is not None: wait_seconds = time_difference(next_wakeup_time, now) self._wakeup.wait(wait_seconds) self._wakeup.clear() elif self.standalone: self.shutdown() break else: self._wakeup.wait() self._wakeup.clear() 进入循环后首先调用 _process_jobs 处理任务，以此处理不同 job_store 中的 每一个 job。在处理 job 过程中首先通过 get_run_times 获取 run_times（get_run_times 很有趣，它获取在 next_run_time 和 now 之间所有需要进行任务调度的时间点，之所以这样做的原因是 APScheduler 允许设定一个 misfire_grace_time 时间，也就是事件执行的延迟时间，因为有很多原因会导致计划调度不能准确在设定好的时间执行。）_process_jobs 处理很简单，将 job 的执行调度交给 scheduler 的线程池，针对每一个 job 的触发会开启一个新的线程（一个疑问：这个线程设置了 t.setDaemon(True)，但是文档上却说”Jobs are always executed in non-daemonic threads.”）来执行，而实际的任务执行发生在 Scheduler 的 _run_job 方法中。 _process_jobs 会返回下次执行调度的时间，调度循环会根据返回值进行相应的处理，wait 指定时间、或一直 wait 等待事件通知唤醒、或退出循环。调度循环的阻塞和唤醒是由 python 原生 Event 的 wait 和 set 来实现的，阻塞结束的方式有两种：一是 wait(wait_seconds) 超时；另一种是在 scheduler 处于 running 状态添加新的任务，添加新任务过程中会自动调用 set()唤醒 event。 总结总体而言 APScheduler 以 threading 模块为基础实现，主要用到了 threading.Event 和 threading.Thread，用到的 ThreadPool 是对 threading.Thread 的简单封装。真是因为此所以 APScheduler 有 “No (hard) external dependencies” 和 “Thread-safe API” 这两项优点。但同时存在一个问题，由于 GIL 的存在，任务的执行一定会阻塞主线程，所以如果任务执行时间较长、有更多异步调度的需求，那么可能就会用到另外一个更强大的框架：Celery。 Celery 毕竟是一个分布式的任务队列，相比而言 APScheduler 的特点是轻巧，一言以蔽之即: APScheduler is a light but powerful in-process task scheduler.","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"task scheduler","slug":"task-scheduler","permalink":"http://amyangfei.me/tags/task-scheduler/"},{"name":"source code reading","slug":"source-code-reading","permalink":"http://amyangfei.me/tags/source-code-reading/"}]},{"title":"xenserver 使用小结","slug":"xenserver-use-tip","date":"2013-09-06T00:00:00.000Z","updated":"2020-12-13T07:05:26.267Z","comments":true,"path":"2013/09/06/xenserver-use-tip/","link":"","permalink":"http://amyangfei.me/2013/09/06/xenserver-use-tip/","excerpt":"手头四台服务器，准备搭建一个小的云平台，了解了几种不同的方案，包括Vmware Esxi，Xen，OpenStack 等。先把各种方法都试一试，然后确定一个具体实施。之前有使用过 Vmware ESXi，虽说 ESXi 可以免费使用，但是 Vmware 浓厚的商业气息让我有种道不同不相为谋的感觉。XenServer 最近有开源，于是昨天花了一个下午尝试了一下。 安装 XenServer 和 XenCenter首先去官网下载 XenServer 的镜像文件，我下载了 6.2.0 版本。XenServer 本身其实就是一个 Linux 操作系统，于是服务器直接通过启动 bios 引导安装 XenServer，安装过程中配置好 root 的密码以及网络信息，这样就可以通过管理工具管理 XenServer 了。 使用的管理工具是 Windows 版的，貌似有 Linux 版本的OpenXenCenter，我没有试验。进入 XenCenter，服务器是四网卡，现在接了0，2号网卡。2号网卡连接外网，配有有一个 166.111.xx.yy的 ip。0号网卡接内部网络，ip 设为 192.168.10.254。配置如下图：","text":"手头四台服务器，准备搭建一个小的云平台，了解了几种不同的方案，包括Vmware Esxi，Xen，OpenStack 等。先把各种方法都试一试，然后确定一个具体实施。之前有使用过 Vmware ESXi，虽说 ESXi 可以免费使用，但是 Vmware 浓厚的商业气息让我有种道不同不相为谋的感觉。XenServer 最近有开源，于是昨天花了一个下午尝试了一下。 安装 XenServer 和 XenCenter首先去官网下载 XenServer 的镜像文件，我下载了 6.2.0 版本。XenServer 本身其实就是一个 Linux 操作系统，于是服务器直接通过启动 bios 引导安装 XenServer，安装过程中配置好 root 的密码以及网络信息，这样就可以通过管理工具管理 XenServer 了。 使用的管理工具是 Windows 版的，貌似有 Linux 版本的OpenXenCenter，我没有试验。进入 XenCenter，服务器是四网卡，现在接了0，2号网卡。2号网卡连接外网，配有有一个 166.111.xx.yy的 ip。0号网卡接内部网络，ip 设为 192.168.10.254。配置如下图： 安装一台虚拟机接下来安装一台虚拟机。可以选择的操作系统比较多，比如选择 Ubuntu Server 12.04，需要注意的是加载虚拟镜像的方法，有两大类：一类是 Install from ISO library or DVD drive，另一类是 Boot from network。第二类我没有试验，第一类又分两种，一种是从光驱加载镜像，由于我在远程操作所以选择了另一种 ISO Library。建立 ISO Library 的方法是在左侧导航选择目标 xenserver 建立 New SR，然后选择 windwos 的文件共享或者是 NFS。选择好安装虚拟机时就可以找到镜像文件了。然后一步步配置好参数，开始运行，继续就是熟悉的 Ubuntu 安装过程。 通过 iptables 进行 NAT安装好一台虚拟机后进行网络配置，首先在 XenCenter 中激活安装虚拟机的0号网卡，然后到 Console 界面配置 ip，比如设定 ip 为192.168.10.11，网关设为 192.168.10.254，OK。接下类需要到 XenServer 中配置 iptables 的 NAT 转发。 iptables中DNAT与SNAT的理解 这篇文章中作者对 iptables NAT 的原理解释比较清楚，其实无论是 NAT 穿透还是端口转发，本质都是在 iptables 处对 ip 数据包的源地址和目标地址的一定规则的转换。我配置了3条 iptables 规则，实现了内部虚拟机可以正常访问外网，同时外网的主机可以通过10022这个端口 ssh 访问内部虚拟机。其实第2条规则被第3条规则包含，可以去掉。 123456# 端口重定向$ iptables -t nat -A PREROUTING -d 166.111.xx.yy -p tcp --dport 10022 -j DNAT --to 192.168.10.11:22$ iptables -t nat -A POSTROUTING -d 192.168.10.11 -p tcp --dport 22 -j SNAT --to 166.111.xx.yy# NAT代理上网$ iptables -t nat -A POSTROUTING -j SNAT --to 166.111.xx.yy 总体过程比较顺利，配置 iptables NAT 折腾了一小会儿。于是，have fun, enjoy it ~","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"virtualization","slug":"virtualization","permalink":"http://amyangfei.me/tags/virtualization/"}]},{"title":"使用 Tornado 进行异步编程","slug":"asynchronous-programming-with-tornado","date":"2013-06-17T00:00:00.000Z","updated":"2020-12-13T07:04:23.610Z","comments":true,"path":"2013/06/17/asynchronous-programming-with-tornado/","link":"","permalink":"http://amyangfei.me/2013/06/17/asynchronous-programming-with-tornado/","excerpt":"翻译自：Asynchronous programming with Tornado 对于初学者来说异步编程很令人迷惑，因此我觉得有必要介绍一些有用的基本概念来帮助初学者避免一些常见的陷阱。如果希望理解通用的异步编程模型，可以查看以下这些网络资源，Introduction to Asynchronous Programming，Twisted Introduction。在这篇文章中我将会着眼于如何使用 Tornado 进行异步编程。 来自Tornado主页的一段话： FriendFeed’s web server is a relatively simple, non-blocking web server written in Python. The FriendFeed application is written using a web framework that looks a bit like web.py or Google’s webapp, but with additional tools and optimizations to take advantage of the non-blocking web server and tools. Tornado is an open source version of this web server and some of the tools we use most often at FriendFeed. The framework is distinct from most mainstream web server frameworks (and certainly most Python frameworks) because it is non-blocking and reasonably fast. Because it is non-blocking and uses epoll or kqueue, it can handle thousands of simultaneous standing connections, which means the framework is ideal for real-time web services. We built the web server specifically to handle FriendFeed’s real-time features every active user of FriendFeed maintains an open connection to the FriendFeed servers. (For more information on scaling servers to support thousands of clients, see The C10K problem.) 对于初学者首先需要认清的是自己是否真的需要异步操作。异步编程比同步编程复杂得多，因此有人说：异步编程是不适合人类大脑的。","text":"翻译自：Asynchronous programming with Tornado 对于初学者来说异步编程很令人迷惑，因此我觉得有必要介绍一些有用的基本概念来帮助初学者避免一些常见的陷阱。如果希望理解通用的异步编程模型，可以查看以下这些网络资源，Introduction to Asynchronous Programming，Twisted Introduction。在这篇文章中我将会着眼于如何使用 Tornado 进行异步编程。 来自Tornado主页的一段话： FriendFeed’s web server is a relatively simple, non-blocking web server written in Python. The FriendFeed application is written using a web framework that looks a bit like web.py or Google’s webapp, but with additional tools and optimizations to take advantage of the non-blocking web server and tools. Tornado is an open source version of this web server and some of the tools we use most often at FriendFeed. The framework is distinct from most mainstream web server frameworks (and certainly most Python frameworks) because it is non-blocking and reasonably fast. Because it is non-blocking and uses epoll or kqueue, it can handle thousands of simultaneous standing connections, which means the framework is ideal for real-time web services. We built the web server specifically to handle FriendFeed’s real-time features every active user of FriendFeed maintains an open connection to the FriendFeed servers. (For more information on scaling servers to support thousands of clients, see The C10K problem.) 对于初学者首先需要认清的是自己是否真的需要异步操作。异步编程比同步编程复杂得多，因此有人说：异步编程是不适合人类大脑的。 如果你的应用需要监控一些资源并且当这些资源的状态发生变化时需要采取一定的操作，那么你需要使用异步编程。比如对于一个 web 服务器，如果没有请求到达，那么它处于空闲状态；当有请求通过 socket 到达 web 服务器它就需要对这条请求进行一定的处理。另外一种需要异步编程的情况比如一个应用需要定期的执行一些任务或者延迟一段时间再执行代码。可以使用多线程/进程来控制多个任务的并发执行，那样编程模型也会迅速变得复杂起来。 第二步是需要确认你想要的操作是否能够进行异步操作。不幸的是在 Tornado 中，并非所有的功能都可以异步执行。 Tornado是单线程运行的（尽管在实际应用中，它支持多线程模式），因此阻塞操作会阻塞整个服务器。这意味着一个阻塞操作将会阻止系统执行下一个等待执行的任务。任务的调度通过 IOLoop 完成，IOLoop运行在唯一的可用的线程中。 下边是一个错误使用 IOLoop 的例子（译者注：这段代码与原文不一样，是按照原文的描述修改的）： 1234567891011121314151617import timefrom tornado.ioloop import IOLoopfrom tornado import gen@gen.enginedef f(): print &#x27;sleeping&#x27; time.sleep(1) print &#x27;awake!&#x27;if __name__ == &quot;__main__&quot;: # Note that now code is executed &quot;concurrently&quot; IOLoop.instance().add_callback(f) IOLoop.instance().add_callback(f) IOLoop.instance().start() 注意到 blocking_call（译者注：函数f，不知道为什么原文作者说这是blocking_call） 被正确地调用，但是由于它被 time.sleep 阻塞，会阻止接下来任务（第二次调用该函数）的执行。只有当第一次调用结束后，这个函数才会被IOLoop 调度第二次调用。因此输出是这样的一个序列（“sleeping”, “awake!”, “sleeping”, “awake!”）。 对比同样的代码，但是使用 time.sleep 的异步版本，例如 add_timeout： 123456789101112131415161718# Example of non-blocking sleep.import timefrom tornado.ioloop import IOLoopfrom tornado import gen@gen.enginedef f(): print &#x27;sleeping&#x27; yield gen.Task(IOLoop.instance().add_timeout, time.time() + 1) print &#x27;awake!&#x27;if __name__ == &quot;__main__&quot;: # Note that now code is executed &quot;concurrently&quot; IOLoop.instance().add_callback(f) IOLoop.instance().add_callback(f) IOLoop.instance().start() 在这种情况下，函数 f 第一次被调用，会打印“sleeping”，然后它会在1秒之后向 IOLoop 请求继续执行。IOLoop 重获控制权，它会调度函数 f 的第二次调用，第二次调用首先打印“sleeping”，之后将控制权还给 IOLoop。1秒钟后 IOLoop 会在第一个函数挂起的位置继续执行并且打印“awake”。最后，第二次“awake”也会被打印。所以全部的打印序列为“sleeping”, “sleeping”, “awake!”, “awake!”。这两次函数调用是并发执行的（但不是并行！） 现在我会听到你提问：“我如何创建一个函数并且异步地执行它？”在 Tornado 中，每一个有“callback”参数的函数都可以使用 “gen.engine.Task（译者注：应该是gen.Task）”进行异步操作。但是要注意：使用 Task 并不意味着就一定是异步执行！一个事实是函数会被调度获得控制权并执行，执行后任何传递给 callback 的值都会在 Task 中返回。看下边的代码： 12345678910111213141516171819202122232425import timefrom tornado.ioloop import IOLoopfrom tornado import gendef my_function(callback): print &#x27;do some work&#x27; # Note: this line will block! time.sleep(1) callback(123)@gen.enginedef f(): print &#x27;start&#x27; # Call my_function and return here as soon as &quot;callback&quot; is called. # &quot;result&quot; is whatever argument was passed to &quot;callback&quot; in &quot;my_function&quot;. result = yield gen.Task(my_function) print &#x27;result is&#x27;, result IOLoop.instance().stop()if __name__ == &quot;__main__&quot;: f() IOLoop.instance().start() 绝大多数初学者可能会这样写：Task(my_func)，然后认为 my_func 会自动被异步执行。事实上这并不是 Tornado 工作的原理，这是 Go 如何工作的！下边是我最后的建议(译者注：我觉得这是这篇文章最重要的建议)： ** In a function that is going to be used “asynchronously”, only asynchronous libraries should be used. ** 就是说如果希望异步编程，那么一些阻塞的调用比如 time.sleep 或者 urllib2.urlopen 或者 db.query，它们需要替换成相应的异步版本。比如，IOLoop.add_timeout 是 time.sleep 的替换，AsyncHTTPClient.fetch 是 urllib2.urlopen 的替换等等。对于数据库查询，情况比较复杂，需要一些特定的异步查询驱动，比如对于 MongoDB 的 Motor 。","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"tornado","slug":"tornado","permalink":"http://amyangfei.me/tags/tornado/"},{"name":"asynchronous programming","slug":"asynchronous-programming","permalink":"http://amyangfei.me/tags/asynchronous-programming/"}]},{"title":"stl make_heap在llvm和g++下的不同实现","slug":"stl-heap-llvm-gcc","date":"2013-05-05T00:00:00.000Z","updated":"2020-12-13T07:05:07.621Z","comments":true,"path":"2013/05/05/stl-heap-llvm-gcc/","link":"","permalink":"http://amyangfei.me/2013/05/05/stl-heap-llvm-gcc/","excerpt":"先来看一段十分简单的使用stl的c++代码， 12345678910111213141516#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main(void) &#123; vector&lt;int&gt; vec; int len = 5; for (int i = 1; i &lt;= len; ++i) vec.push_back(i); make_heap(vec.begin(), vec.end()); for (vector&lt;int&gt;::iterator iter = vec.begin(); iter != vec.end(); iter++) &#123; cout &lt;&lt; *iter &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; return 0;&#125; 发现在 xcode 中运行和在 mac terminal 中利用 g++ 编译运行结果不一样，分别为： 12xcode-default : 5 4 2 1 3terminal-g++ : 5 4 3 1 2","text":"先来看一段十分简单的使用stl的c++代码， 12345678910111213141516#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main(void) &#123; vector&lt;int&gt; vec; int len = 5; for (int i = 1; i &lt;= len; ++i) vec.push_back(i); make_heap(vec.begin(), vec.end()); for (vector&lt;int&gt;::iterator iter = vec.begin(); iter != vec.end(); iter++) &#123; cout &lt;&lt; *iter &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; return 0;&#125; 发现在 xcode 中运行和在 mac terminal 中利用 g++ 编译运行结果不一样，分别为： 12xcode-default : 5 4 2 1 3terminal-g++ : 5 4 3 1 2 建堆产生不同结果的原因很简单，即两次编译代码所使用的编译器不同，xcode中默认使用”Apple LLVM compiler”，在g++中使用的是”LLVM GCC”。而这两种不同编译器选择的STL实现方法是不一样的，LLVM默认选择”libc++(LLVM C++ standard library)”，而g++默认使用的是”libstdc++(GNU C++ standard library)”。 libc++中的实现在libc++中make_heap的实现如下所示，从代码中可以看出，通过 __last 这个随机访问的迭代器，从前向后遍历，调用 __push_heap_back 将数据依次插入到堆中。__push_heap_back 的实现是将新插入元素放在堆尾，然后针对这个元素使用 shift up 策略调整至合适位置。 1234567891011121314template &lt;class _Compare, class _RandomAccessIterator&gt;void__make_heap(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp)&#123; typedef typename iterator_traits&lt;_RandomAccessIterator&gt;::difference_type difference_type; difference_type __n = __last - __first; if (__n &gt; 1) &#123; __last = __first; ++__last; for (difference_type __i = 1; __i &lt; __n;) __push_heap_back&lt;_Compare&gt;(__first, ++__last, __comp, ++__i); &#125;&#125; libstdc++中的实现在 libstdc++ 中 make_heap 首先将所有元素按照原顺序放入堆的存储结构，然后从最大的非叶子节点开始调整元素位置，即调用 __adjust_heap 操作，__adjust_heap 会自上向下依次选择每个子节点中较大的元素上升。 1234567891011121314151617181920212223242526template&lt;typename _RandomAccessIterator&gt; void make_heap(_RandomAccessIterator __first, _RandomAccessIterator __last) &#123; typedef typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type _ValueType; typedef typename iterator_traits&lt;_RandomAccessIterator&gt;::difference_type _DistanceType; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt;_RandomAccessIterator&gt;) __glibcxx_function_requires(_LessThanComparableConcept&lt;_ValueType&gt;) __glibcxx_requires_valid_range(__first, __last); if (__last - __first &lt; 2) return; const _DistanceType __len = __last - __first; _DistanceType __parent = (__len - 2) / 2; while (true) &#123; std::__adjust_heap(__first, __parent, __len, _ValueType(*(__first + __parent))); if (__parent == 0) return; __parent--; &#125; &#125; 注：以上结果是在 OS X 10.8.4，Xcode4 下的测试结果。不同版本编译器结果会有不同。","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://amyangfei.me/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://amyangfei.me/tags/stl/"},{"name":"llvm","slug":"llvm","permalink":"http://amyangfei.me/tags/llvm/"}]},{"title":"tornado源码分析5","slug":"tornado-source-analysis-5","date":"2013-03-01T00:00:00.000Z","updated":"2020-12-13T07:05:23.408Z","comments":true,"path":"2013/03/01/tornado-source-analysis-5/","link":"","permalink":"http://amyangfei.me/2013/03/01/tornado-source-analysis-5/","excerpt":"Tornado的web框架在web.py中实现，主要包括RequestHandler类（本质为对http请求处理的封装）和Application类（是一些列请求处理的集合，构成的一个web-application，源代码注释不翻译更容易理解：A collection of request handlers that make up a web application）。 RequestHandler分析RequestHandler提供了一个针对http请求处理的基类封装，方法比较多，主要有以下功能： 提供了GET/HEAD/POST/DELETE/PATCH/PUT/OPTIONS等方法的功能接口，具体开发时RequestHandler的子类重写这些方法以支持不同需求的请求处理。 提供对http请求的处理方法，包括对headers，页面元素，cookie的处理。 提供对请求响应的一些列功能，包括redirect，write（将数据写入输出缓冲区），渲染模板（render, reander_string）等 其他的一些辅助功能，如结束请求/响应，刷新输出缓冲区，对用户授权相关处理等。","text":"Tornado的web框架在web.py中实现，主要包括RequestHandler类（本质为对http请求处理的封装）和Application类（是一些列请求处理的集合，构成的一个web-application，源代码注释不翻译更容易理解：A collection of request handlers that make up a web application）。 RequestHandler分析RequestHandler提供了一个针对http请求处理的基类封装，方法比较多，主要有以下功能： 提供了GET/HEAD/POST/DELETE/PATCH/PUT/OPTIONS等方法的功能接口，具体开发时RequestHandler的子类重写这些方法以支持不同需求的请求处理。 提供对http请求的处理方法，包括对headers，页面元素，cookie的处理。 提供对请求响应的一些列功能，包括redirect，write（将数据写入输出缓冲区），渲染模板（render, reander_string）等 其他的一些辅助功能，如结束请求/响应，刷新输出缓冲区，对用户授权相关处理等。 Application分析源代码中的注释写的非常好：A collection of request handlers that make up a web application. Instances of this class are callable and can be passed directly to HTTPServer to serve the application. 该类初始化的第一个参数接受一个(regexp, request_class)形式的列表，指定了针对不同URL请求所采取的处理方法，包括对静态文件请求的处理（web.StaticFileHandler）。Application类中实现 __call__ 函数，这样该类就成为可调用的对象，由HTTPServer来进行调用。比如下边是httpserver.py中HTTPConection类的代码，该处request_callback即为Application对象。 12345def _on_headers(self, data): # some codes... self.request_callback(self._request) __call__函数会遍历Application的handlers列表，匹配到相应的URL后通过handler._execute进行相应处理；如果没有匹配的URL，则会调用ErrorHandler。 在Application初始时有一个debug参数，当debug=True时，运行程序时当有代码、模块发生修改，程序会自动重新加载，即实现了auto-reload功能。该功能在autoreload.py文件中实现，是否需要reload的检查在每次接收到http请求时进行，基本原理是检查每一个sys.modules以及_watched_files所包含的模块在程序中所保存的最近修改时间和文件系统中的最近修改时间是否一致，如果不一致，则整个程序重新加载。 123456def _reload_on_update(modify_times): for module in sys.modules.values(): # module test and some path handles _check_file(modify_times, path) for path in _watched_files: _check_file(modify_times, path) Tornado的autoreload模块提供了一个对外的main接口，可以通过下边的方法实现运行test.py程序运行的auto-reload。但是测试了一下，功能有限，相比于django的autorelaod模块（具有较好的封装和较完善的功能）还是有一定的差距。最主要的原因是Tornado中的实现耦合了一些ioloop的功能，因而autoreload不是一个可独立的模块。 123456# tornadopython -m tornado.autoreload test.py [args...]# djangofrom django.utils import autoreloadautoreload.main(your-main-func) asynchronous方法该方法通常被用为请求处理函数的decorator，以实现异步操作，被@asynchronous修饰后的请求处理为长连接，在调用self.finish之前会一直处于连接等待状态。 总结tornado源码分析2 一文中给出了一张tornado httpserver的工作流程图，调用Application发生在HTTPConnection大方框的handle_request椭圆中。那篇文章里使用的是一个简单的请求处理函数handle_request，无论是handle_request还是application，其本质是一个函数（可调用的对象），当服务器接收连接并读取http请求header之后进行调用，进行请求处理和应答。 12http_server = httpserver.HTTPServer(handle_request)http_server = httpserver.HTTPServer(application)","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"tornado","slug":"tornado","permalink":"http://amyangfei.me/tags/tornado/"},{"name":"web framework","slug":"web-framework","permalink":"http://amyangfei.me/tags/web-framework/"}]},{"title":"Android运行脚本与定时工具","slug":"android-taskscheduler-and-scripting","date":"2013-02-08T00:00:00.000Z","updated":"2020-12-13T07:04:14.637Z","comments":true,"path":"2013/02/08/android-taskscheduler-and-scripting/","link":"","permalink":"http://amyangfei.me/2013/02/08/android-taskscheduler-and-scripting/","excerpt":"用惯了crontab，希望在自己的Android手机上完成定时执行一些脚本的任务。google一下找到了 android应用实现定时打电话 这样一篇文章，正好满足我的需求。下面做一个简单的总结。 安装的软件 SL4A(Scripting Layer for Android)，Andriod系统下运行脚本的环境，可以在终端、后台或Locale中运行，现阶段支持Python, Perl, JRuby, Lua, BeanShell, JavaScript, Tcl和shell脚本。 Py4A，SL4A的python插件，安装之后就可以运行python脚本。 TaskBomb task scheduler，一个可以执行计划任务的app，类似于Unix中的crontab。 SL4A Script Launcher，TaskBomb可以通过此app执行SL4A脚本。","text":"用惯了crontab，希望在自己的Android手机上完成定时执行一些脚本的任务。google一下找到了 android应用实现定时打电话 这样一篇文章，正好满足我的需求。下面做一个简单的总结。 安装的软件 SL4A(Scripting Layer for Android)，Andriod系统下运行脚本的环境，可以在终端、后台或Locale中运行，现阶段支持Python, Perl, JRuby, Lua, BeanShell, JavaScript, Tcl和shell脚本。 Py4A，SL4A的python插件，安装之后就可以运行python脚本。 TaskBomb task scheduler，一个可以执行计划任务的app，类似于Unix中的crontab。 SL4A Script Launcher，TaskBomb可以通过此app执行SL4A脚本。 简单使用 安装Py4A后进入程序，点击最上边的Install会自动下载Python运行所需要的类库。 进入SL4A会看到在SL4A程序Scripts目录下的脚本，可以在terminal或后台运行这些脚本；在SL4A中可以添加、修改、删除脚本程序，使用方法很简单，不详细说明。 需要注意的是Py4A支持的是python2.6，最新的python-for-android的项目地址已经在 这里 。Py4A不仅支持python的原生API，同时可以很方便的调用Android的系统API，比如可以使用下边很简单的代码向目标手机发送短信。详细可参考 ApiReference。 1234import androiddroid = android.Android()droid.smsSend(&#x27;15120000xxx&#x27;, &#x27;this is an sms auto sent by Py4A.&#x27;) 其他最后这部分就是与文章内容不相关了。是一个虾米音乐自动签到的程序。需要注意的就是登录时一定要将headers中的User-Agent修改为浏览器的User-Agent，签到的post请求需要修改headers中的User-Agent和Referer地址。其他就没有什么了（突然想到和某只喵喵的比赛是不是可以拿TaskBomb+SL4A作弊），各位看官求轻拍，捂面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-import urllib2, urllibdef login(email, password): try: cookies = urllib2.HTTPCookieProcessor() opener = urllib2.build_opener(cookies) urllib2.install_opener(opener) params = &#123;&#x27;email&#x27;:email, &#x27;password&#x27;:password, &#x27;submit&#x27;:&#x27;登 录&#x27;&#125; loginUrl = &#x27;http://www.xiami.com/member/login&#x27; headers = &#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.57 Safari/537.17&quot;, &#125; request = urllib2.Request( url = loginUrl, headers = headers, data = urllib.urlencode(params) ) login = urllib2.urlopen(request) print &#x27;login successfully...&#x27; return login except Exception, e: print &#x27;login failed:&#x27;, e return Nonedef signin(email, password): signurl = &#x27;http://www.xiami.com/task/signin&#x27; if login(email, password) != None: try: headers = &#123; &quot;Referer&quot;: &quot;http://www.xiami.com/?register&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.57 Safari/537.17&quot;, &#125; request = urllib2.Request( url = signurl, headers = headers ) response = urllib2.urlopen(request) retcode = response.read() if str.isdigit(retcode): print &#x27;sign in days:&#x27;, retcode except Exception, e: print &#x27;signin error:&#x27;, eif __name__ == &#x27;__main__&#x27;: email = &#x27;xxx&#x27; password = &#x27;xxx&#x27; signin(email, password) 参考android应用实现定时打电话， google code SL4A","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"android","slug":"android","permalink":"http://amyangfei.me/tags/android/"}]},{"title":"tornado源码分析4","slug":"tornado-source-analysis-4","date":"2013-02-05T00:00:00.000Z","updated":"2020-12-13T07:05:20.414Z","comments":true,"path":"2013/02/05/tornado-source-analysis-4/","link":"","permalink":"http://amyangfei.me/2013/02/05/tornado-source-analysis-4/","excerpt":"IOStream对socket读写进行了封装，分别提供读、写缓冲区实现对socket的异步读写。当socket被accept之后HTTPServer的_handle_connection会被回调并初始化IOStream对象，进一步通过IOStream提供的功能接口完成socket的读写。文章接下来将关注IOStream实现读写的细节。 IOStream的初始化IOStream初始化过程中主要完成以下操作： 绑定对应的socket 绑定ioloop 创建读缓冲区_read_buffer，一个python deque容器 创建写缓冲区_write_buffer，同样也是一个python deque容器","text":"IOStream对socket读写进行了封装，分别提供读、写缓冲区实现对socket的异步读写。当socket被accept之后HTTPServer的_handle_connection会被回调并初始化IOStream对象，进一步通过IOStream提供的功能接口完成socket的读写。文章接下来将关注IOStream实现读写的细节。 IOStream的初始化IOStream初始化过程中主要完成以下操作： 绑定对应的socket 绑定ioloop 创建读缓冲区_read_buffer，一个python deque容器 创建写缓冲区_write_buffer，同样也是一个python deque容器 IOStream提供的主要功能接口主要的读写接口包括以下四个： class IOStream(object): &nbsp;&nbsp;&nbsp;&nbsp;def read_until(self, delimiter, callback): &nbsp;&nbsp;&nbsp;&nbsp;def read_bytes(self, num_bytes, callback, streaming_callback=None): &nbsp;&nbsp;&nbsp;&nbsp;def read_until_regex(self, regex, callback): &nbsp;&nbsp;&nbsp;&nbsp;def read_until_close(self, callback, streaming_callback=None): &nbsp;&nbsp;&nbsp;&nbsp;def write(self, data, callback=None): read_until和read_bytes是最常用的读接口，它们工作的过程都是先注册读事件结束时调用的回调函数，然后调用_try_inline_read方法。_try_inline_read首先尝试_read_from_buffer，即从上一次的读缓冲区中取数据，如果有数据直接调用 self._run_callback(callback, self._consume(data_length)) 执行回调函数，_consume消耗掉了_read_buffer中的数据；否则即_read_buffer之前没有未读数据，先通过_read_to_buffer将数据从socket读入_read_buffer，然后再执行_read_from_buffer操作。read_until和read_bytes的区别在于_read_from_buffer过程中截取数据的方法不同，read_until读取到delimiter终止，而read_bytes则读取num_bytes个字节终止。执行过程如下图所示： read_until_regex相当于delimiter为某一正则表达式的read_until。 read_until_close主要用于IOStream流关闭前后的读取：如果调用read_until_close时stream已经关闭，那么将会_consume掉_read_buffer中的所有数据；否则_read_until_close标志位设为True，注册_streaming_callback回调函数，调用_add_io_state添加io_loop.READ状态。 write首先将data按照数据块大小WRITE_BUFFER_CHUNK_SIZE分块写入_write_buffer，然后调用_handle_write向socket发送数据。 其他内部功能接口 def _handle_events(self, fd, events): 通常为IOLoop对象add_handler方法传入的回调函数，由IOLoop的事件机制来进行调度。 def _add_io_state(self, state): 为IOLoop对象的handler注册IOLoop.READ或IOLoop.WRITE状态，handler为IOStream对象的_handle_events方法。 def _consume(self, loc): 合并读缓冲区loc个字节，从读缓冲区删除并返回这些数据 参考Tornado源码分析之http服务器篇， tornado源码分析系列","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"tornado","slug":"tornado","permalink":"http://amyangfei.me/tags/tornado/"},{"name":"asynchronous I/O","slug":"asynchronous-I-O","permalink":"http://amyangfei.me/tags/asynchronous-I-O/"}]},{"title":"tornado源码分析3","slug":"tornado-source-analysis-3","date":"2013-02-03T00:00:00.000Z","updated":"2020-12-13T07:05:17.415Z","comments":true,"path":"2013/02/03/tornado-source-analysis-3/","link":"","permalink":"http://amyangfei.me/2013/02/03/tornado-source-analysis-3/","excerpt":"注：在分割线之前是基于 Tornado2.4 的分析。在Tornado3.0+以后IOLoop发生了一些改动，分割线之后有相应的介绍。 IOLoop是基于epoll实现的底层网络I/O的核心调度模块，用于处理socket相关的连接、响应、异步读写等网络事件。每个Tornado进程都会初始化一个全局唯一的IOLoop实例，在IOLoop中通过静态方法instance()进行封装，获取IOLoop实例直接调用此方法即可。 12345678910111213@staticmethoddef instance(): &quot;&quot;&quot; class MyClass(object): def __init__(self, io_loop=None): self.io_loop = io_loop or IOLoop.instance() &quot;&quot;&quot; if not hasattr(IOLoop, &quot;_instance&quot;): with IOLoop._instance_lock: if not hasattr(IOLoop, &quot;_instance&quot;): # New instance after double check IOLoop._instance = IOLoop() return IOLoop._instance","text":"注：在分割线之前是基于 Tornado2.4 的分析。在Tornado3.0+以后IOLoop发生了一些改动，分割线之后有相应的介绍。 IOLoop是基于epoll实现的底层网络I/O的核心调度模块，用于处理socket相关的连接、响应、异步读写等网络事件。每个Tornado进程都会初始化一个全局唯一的IOLoop实例，在IOLoop中通过静态方法instance()进行封装，获取IOLoop实例直接调用此方法即可。 12345678910111213@staticmethoddef instance(): &quot;&quot;&quot; class MyClass(object): def __init__(self, io_loop=None): self.io_loop = io_loop or IOLoop.instance() &quot;&quot;&quot; if not hasattr(IOLoop, &quot;_instance&quot;): with IOLoop._instance_lock: if not hasattr(IOLoop, &quot;_instance&quot;): # New instance after double check IOLoop._instance = IOLoop() return IOLoop._instance 在上一篇文章中已经分析Tornado服务器启动时会创建监听socket，并将socket的file descriptor注册到IOLoop实例中，IOLoop添加对socket的IOLoop.READ事件监听并传入回调处理函数。当某个socket通过accept接受连接请求后调用注册的回调函数进行读写。接下来主要分析IOLoop对epoll的封装和I/O调度具体实现。 epoll是Linux内核中实现的一种可扩展的I/O事件通知机制，是对POISX系统中 select(2) 和 poll(2) 的替代，具有更高的性能和扩展性，FreeBSD中类似的实现是kqueue。Tornado中基于Python C扩展实现的的epoll模块(或kqueue)对epoll(kqueue)的使用进行了封装，使得IOLoop对象可以通过相应的事件处理机制对I/O进行调度。 IOLoop模块对网络事件类型的封装与epoll一致，分为READ，WRITE， ERROR三类，具体如下所示。 123READ = _EPOLLINWRITE = _EPOLLOUTERROR = _EPOLLERR | _EPOLLHUP IOLoop的初始化初始化过程中选择epoll的实现方式，Linux平台为epoll，BSD平台为kqueue，其他平台如果安装有C模块扩展的epoll则使用tornado对epoll的封装，否则退化为select。 123456789101112def __init__(self, impl=None): self._impl = impl or _poll() #省略部分代码 self._waker = Waker() self.add_handler(self._waker.fileno(), lambda fd, events: self._waker.consume(), self.READ)def add_handler(self, fd, handler, events): &quot;&quot;&quot;Registers the given handler to receive the given events for fd.&quot;&quot;&quot; self._handlers[fd] = stack_context.wrap(handler) self._impl.register(fd, events | self.ERROR) 在IOLoop初始化的过程中创建了一个Waker对象，将Waker对象fd的读端注册到事件循环中并设定相应的回调函数（这样做的好处是当事件循环阻塞而没有响应描述符出现，需要在最大timeout时间之前返回，就可以向这个管道发送一个字符）。Waker的使用：一种是在其他线程向IOLoop添加callback时使用，唤醒IOLoop同时会将控制权转移给IOLoop线程并完成特定请求。唤醒的方法向管道中写入一个字符’x’。另外，在IOLoop的stop函数中会调用self._waker.wake()，通过向管道写入’x’停止事件循环。 add_handler函数使用了stack_context提供的wrap方法。wrap返回了一个可以直接调用的对象并且保存了传入之前的堆栈信息，在执行时可以恢复，这样就保证了函数的异步调用时具有正确的运行环境。 IOLoop的start方法IOLoop的核心调度集中在start方法中，IOLoop实例对象调用start后开始epoll事件循环机制，该方法会一直运行直到IOLoop对象调用stop函数、当前所有事件循环完成。start方法中主要分三个部分：一个部分是对超时的相关处理；一部分是epoll事件通知阻塞、接收；一部分是对epoll返回I/O事件的处理。 为防止IO event starvation，将回调函数延迟到下一轮事件循环中执行。 超时的处理heapq维护一个最小堆，记录每个回调函数的超时时间（deadline）。每次取出deadline最早的回调函数，如果callback标志位为True并且已经超时，通过_run_callback调用函数；如果没有超时需要重新设定poll_timeout的值。 通过self._impl.poll(poll_timeout)进行事件阻塞，当有事件通知或超时时poll返回特定的event_pairs。 epoll返回通知事件后将新事件加入待处理队列，将就绪事件逐个弹出，通过stack_context.wrap(handler)保存的可执行对象调用事件处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950while True: poll_timeout = 3600.0 with self._callback_lock: callbacks = self._callbacks self._callbacks = [] for callback in callbacks: self._run_callback(callback) # 超时处理 if self._timeouts: now = time.time() while self._timeouts: if self._timeouts[0].callback is None: # the timeout was cancelled heapq.heappop(self._timeouts) elif self._timeouts[0].deadline &lt;= now: timeout = heapq.heappop(self._timeouts) self._run_callback(timeout.callback) else: seconds = self._timeouts[0].deadline - now poll_timeout = min(seconds, poll_timeout) break if self._callbacks: # If any callbacks or timeouts called add_callback, # we don&#x27;t want to wait in poll() before we run them. poll_timeout = 0.0 if not self._running: break if self._blocking_signal_threshold is not None: # clear alarm so it doesn&#x27;t fire while poll is waiting for events. signal.setitimer(signal.ITIMER_REAL, 0, 0) # epoll阻塞，当有事件通知或超时返回event_pairs try: event_pairs = self._impl.poll(poll_timeout) except Exception, e: # 异常处理，省略 # 对epoll返回event_pairs事件的处理 self._events.update(event_pairs) while self._events: fd, events = self._events.popitem() try: self._handlers[fd](fd, events) except Exception e: # 异常处理，省略 至此IOLoop模块的分析基本完成。下一篇文章将会继续分析IOStream模块。 ————————————————我是分割线————————————————— 补充于2013年4月30日，介绍Tornado3.0以后IOLoop模块的一些改动。 1. IOLoop成为util.Configurable的子类，IOLoop 中绝大多数成员方法都作为抽象接口，具体实现由派生类 PollIOLoop 完成。IOLoop实现了 Configurable 中的 configurable_base 和 configurable_default 这两个抽象接口，用于初始化过程中获取类类型和类的实现方法（即 IOLoop 中 poller 的实现方式）。在Tornado3.0+ 中针对不同平台，单独出 poller 相应的实现，EPollIOLoop、KQueueIOLoop、SelectIOLoop 均继承于 PollIOLoop。下边的代码是 configurable_default 方法根据平台选择相应的 epoll 实现。初始化 IOLoop 的过程中会自动根据平台选择合适的 poller 的实现方法。 1234567891011@classmethoddef configurable_default(cls): if hasattr(select, &quot;epoll&quot;): from tornado.platform.epoll import EPollIOLoop return EPollIOLoop if hasattr(select, &quot;kqueue&quot;): # Python 2.6+ on BSD or Mac from tornado.platform.kqueue import KQueueIOLoop return KQueueIOLoop from tornado.platform.select import SelectIOLoop return SelectIOLoop 2. 其他有很多细节上的改动，详细可参见官方文档What’s new in Tornado 3.0 参考Tornado源码分析之http服务器篇， tornado源码分析系列","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"tornado","slug":"tornado","permalink":"http://amyangfei.me/tags/tornado/"},{"name":"asynchronous I/O","slug":"asynchronous-I-O","permalink":"http://amyangfei.me/tags/asynchronous-I-O/"}]},{"title":"tornado源码分析2","slug":"tornado-source-analysis-2","date":"2013-01-29T00:00:00.000Z","updated":"2020-12-13T07:05:14.428Z","comments":true,"path":"2013/01/29/tornado-source-analysis-2/","link":"","permalink":"http://amyangfei.me/2013/01/29/tornado-source-analysis-2/","excerpt":"httpserver.py中给出了一个简单的http服务器的demo，代码如下所示： 12345678910111213from tornado import httpserverfrom tornado import ioloopdef handle_request(request): message = &quot;You requested %s\\n&quot; % request.uri request.write(&quot;HTTP/1.1 200 OK\\r\\nContent-Length: %d\\r\\n\\r\\n%s&quot; % ( len(message), message)) request.finish()http_server = httpserver.HTTPServer(handle_request)http_server.bind(8888)http_server.start()ioloop.IOLoop.instance().start() 该http服务器主要使用到IOLoop, IOStream, HTTPServer, HTTPConnection几大模块，分别在代码ioloop.py, iostream.py, httpserver.py中实现。工作的流程如下图所示：","text":"httpserver.py中给出了一个简单的http服务器的demo，代码如下所示： 12345678910111213from tornado import httpserverfrom tornado import ioloopdef handle_request(request): message = &quot;You requested %s\\n&quot; % request.uri request.write(&quot;HTTP/1.1 200 OK\\r\\nContent-Length: %d\\r\\n\\r\\n%s&quot; % ( len(message), message)) request.finish()http_server = httpserver.HTTPServer(handle_request)http_server.bind(8888)http_server.start()ioloop.IOLoop.instance().start() 该http服务器主要使用到IOLoop, IOStream, HTTPServer, HTTPConnection几大模块，分别在代码ioloop.py, iostream.py, httpserver.py中实现。工作的流程如下图所示： 服务器的工作流程：首先按照socket-&gt;bind-&gt;listen顺序创建listen socket监听客户端，并将每个listen socket的fd注册到IOLoop的单例实例中；当listen socket可读时回调_handle_events处理客户端请求；在与客户端通信的过程中使用IOStream封装了读、写缓冲区，实现与客户端的异步读写。 HTTPServer分析HTTPServer在httpserver.py中实现，继承自TCPServer（netutil.py中实现），是一个无阻塞、单线程HTTP服务器。支持HTTP/1.1协议keep-alive连接，但不支持chunked encoding。服务器支持’X-Real-IP’和’X-Scheme’头以及SSL传输，支持多进程为prefork模式实现。在源代码的注释中对以上描述比较详细的说明，这里就不再细说。 HTTPServer和TCPServer的类结构 class HTTPServer(TCPServer): &nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, request_callback, no_keep_alive=False, io_loop=None, xheaders=False, ssl_options=None, **kwargs): &nbsp;&nbsp;&nbsp;&nbsp;def handle_stream(self, stream, address): class TCPServer(object): &nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, io_loop=None, ssl_options=None): &nbsp;&nbsp;&nbsp;&nbsp;def listen(self, port, address=\"\"): &nbsp;&nbsp;&nbsp;&nbsp;def add_sockets(self, sockets): &nbsp;&nbsp;&nbsp;&nbsp;def bind(self, port, address=None, family=socket.AF_UNSPEC, backlog=128): &nbsp;&nbsp;&nbsp;&nbsp;def start(self, num_processes=1): &nbsp;&nbsp;&nbsp;&nbsp;def stop(self): &nbsp;&nbsp;&nbsp;&nbsp;def handle_stream(self, stream, address): &nbsp;&nbsp;&nbsp;&nbsp;def _handle_connection(self, connection, address): 文章开始部分创建HTTPServer的过程：首先需要定义处理request的回调函数，在tornado中通常使用tornado.web.Application封装。然后构造HTTPServer实例，注册回调函数。接下来监听端口，启动服务器。最后启动IOLoop。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def listen(self, port, address=&quot;&quot;): sockets = bind_sockets(port, address=address) self.add_sockets(sockets)def bind_sockets(port, address=None, family=socket.AF_UNSPEC, backlog=128): # 省略sockets创建，address，flags处理部分代码 for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM, 0, flags)): af, socktype, proto, canonname, sockaddr = res # 创建socket sock = socket.socket(af, socktype, proto) # 设置socket属性，代码省略 sock.bind(sockaddr) sock.listen(backlog) sockets.append(sock) return socketsdef add_sockets(self, sockets): if self.io_loop is None: self.io_loop = IOLoop.instance() for sock in sockets: self._sockets[sock.fileno()] = sock add_accept_handler(sock, self._handle_connection, io_loop=self.io_loop)def add_accept_handler(sock, callback, io_loop=None): if io_loop is None: io_loop = IOLoop.instance() def accept_handler(fd, events): while True: try: connection, address = sock.accept() except socket.error, e: if e.args[0] in (errno.EWOULDBLOCK, errno.EAGAIN): return raise # 当有连接被accepted时callback会被调用 callback(connection, address) io_loop.add_handler(sock.fileno(), accept_handler, IOLoop.READ)def _handle_connection(self, connection, address): # SSL部分省略 try: stream = IOStream(connection, io_loop=self.io_loop) self.handle_stream(stream, address) except Exception: logging.error(&quot;Error in connection callback&quot;, exc_info=True) 这里分析HTTPServer通过listen函数启动监听，这种方法是单进程模式。另外可以通过先后调用bind和start(num_processes=1)函数启动监听同时创建多进程服务器实例，后文有关于此的详细描述。 bind_sockets在启动监听端口过程中调用，getaddrinfo返回服务器的所有网卡信息, 每块网卡上都要创建监听客户端的请求并返回创建的sockets。创建socket过程中绑定地址和端口，同时设置了fcntl.FD_CLOEXEC（创建子进程时关闭打开的socket）和socket.SO_REUSEADDR（保证某一socket关闭后立即释放端口，实现端口复用）标志位。sock.listen(backlog=128)默认设定等待被处理的连接最大个数为128。 返回的每一个socket都加入到IOLoop中同时添加回调函数_handle_connection，IOLoop添加对相应socket的IOLoop.READ事件监听。_handle_connection在接受客户端的连接处理结束之后会被调用，调用时传入连接和ioloop对象初始化IOStream对象，用于对客户端的异步读写；然后调用handle_stream，传入创建的IOStream对象初始化一个HTTPConnection对象，HTTPConnection封装了IOStream的一些操作，用于处理HTTPRequest并返回。至此HTTP Server的创建、启动、注册回调函数的过程分析结束。 HTTPConnection分析该类用于处理http请求。在HTTPConnection初始化时对self.request_callback赋值为一个可调用的对象（该对象用于对http请求的具体处理和应答）。该类首先读取http请求中header的结束符b(“\\r\\n\\r\\n”)，然后回调self._on_headers函数。request_callback的相关实现在以后的系列中有详细介绍。 1234567891011def __init__(self, stream, address, request_callback, no_keep_alive=False, xheaders=False): self.request_callback = request_callback # some configuration code self._header_callback = stack_context.wrap(self._on_headers) self.stream.read_until(b(&quot;\\r\\n\\r\\n&quot;), self._header_callback)def _on_headers(self, data): # some codes self.request_callback(self._request) 多进程HTTPServerTornado的HTTPServer是单进程单线程模式，同时提供了创建多进程服务器的接口，具体实现是在主进程启动HTTPServer时通过process.fork_processes(num_processes)产生新的服务器子进程，所有进程之间共享端口。fork_process的方法在process.py中实现，十分简洁。从开源代码学习Python之tornado的多进程 对fork_process有详细的分析。 FriendFeed使用nginx提供负载均衡、反向代理服务并作为静态文件服务器，在后端服务器上可以部署多个Tornado实例。v2ex:Tornado 项目都是如何部署 里讨论的方案是通过Supervisor控制Tornado app，然后再通过nginx对Tornado的输出进行反向代理。Tornado + Supervisor 在生产环境下的部署方法 这篇文章也有相似的讨论。 参考Tornado源码分析之http服务器篇， tornado源码分析系列， Understanding the code inside Tornado, the asynchronous web server","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"tornado","slug":"tornado","permalink":"http://amyangfei.me/tags/tornado/"},{"name":"web server framework","slug":"web-server-framework","permalink":"http://amyangfei.me/tags/web-server-framework/"}]},{"title":"tornado源码分析1","slug":"tornado-source-analysis-1","date":"2013-01-27T00:00:00.000Z","updated":"2020-12-13T07:05:10.748Z","comments":true,"path":"2013/01/27/tornado-source-analysis-1/","link":"","permalink":"http://amyangfei.me/2013/01/27/tornado-source-analysis-1/","excerpt":"引言Tornado是FriendFeed最早使用的一款由python编写的轻量级、无阻塞式Web服务器，还包括一些相关的工具和优化。现已由Facebook开源在github。得益于无阻塞IO和epoll（or kqueue in FreeBSD, Mac OS X）的使用，Tornado每秒可以处理大量/数千的客户端连接，适用于实时的Web服务（详细可以参阅The C10K problem）。 接下来几篇文章将会从Web服务器框架设计、代码实现细节等角度介绍我对Tornado源码的分析。这篇文章作为概述，首先介绍Tornado的模块按功能分类，同时提供后续文章的结构目录。","text":"引言Tornado是FriendFeed最早使用的一款由python编写的轻量级、无阻塞式Web服务器，还包括一些相关的工具和优化。现已由Facebook开源在github。得益于无阻塞IO和epoll（or kqueue in FreeBSD, Mac OS X）的使用，Tornado每秒可以处理大量/数千的客户端连接，适用于实时的Web服务（详细可以参阅The C10K problem）。 接下来几篇文章将会从Web服务器框架设计、代码实现细节等角度介绍我对Tornado源码的分析。这篇文章作为概述，首先介绍Tornado的模块按功能分类，同时提供后续文章的结构目录。 Tornado模块分类 Core web framework tornado.web — 包含web框架的大部分主要功能，包含RequestHandler和Application两个重要的类 tornado.httpserver — 一个无阻塞HTTP服务器的实现 tornado.template — 模版系统 tornado.escape — HTML,JSON,URLs等的编码解码和一些字符串操作 tornado.locale — 国际化支持 Asynchronous networking 底层模块 tornado.ioloop — 核心的I/O循环 tornado.iostream — 对非阻塞式的 socket 的简单封装，以方便常用读写操作 tornado.httpclient — 一个无阻塞的HTTP服务器实现 tornado.netutil — 一些网络应用的实现，主要实现TCPServer类 Integration with other services tornado.auth — 使用OpenId和OAuth进行第三方登录 tornado.database — 简单的MySQL服务端封装 tornado.platform.twisted — 在Tornado上运行为Twisted实现的代码 tornado.websocket — 实现和浏览器的双向通信 tornado.wsgi — 与其他python网络框架/服务器的相互操作 Utilities tornado.autoreload — 生产环境中自动检查代码更新 tornado.gen — 一个基于生成器的接口，使用该模块保证代码异步运行 tornado.httputil — 分析HTTP请求内容 tornado.options — 解析终端参数 tornado.process — 多进程实现的封装 tornado.stack_context — 用于异步环境中对回调函数的上下文保存、异常处理 tornado.testing — 单元测试 系列文章结构 概述 Tornado HTTP服务器介绍 I/O核心调度模块IOLoop 读写缓冲模块IOStream 核心web框架介绍 未完待续 参考Tornado Documentation, Tornado Documentation CN","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"web server framework","slug":"web-server-framework","permalink":"http://amyangfei.me/tags/web-server-framework/"},{"name":"asynchronous","slug":"asynchronous","permalink":"http://amyangfei.me/tags/asynchronous/"}]},{"title":"bash subshell变量作用域问题","slug":"abtest-script-and-bash-subshell","date":"2013-01-07T00:00:00.000Z","updated":"2020-12-13T07:04:07.378Z","comments":true,"path":"2013/01/07/abtest-script-and-bash-subshell/","link":"","permalink":"http://amyangfei.me/2013/01/07/abtest-script-and-bash-subshell/","excerpt":"今天研究apache ab这个测试工具，在网上看到压力测试shell脚本一文介绍了一个封装的bash脚本，用于多次测试返回requests per second的平均值，对脚本进行了简单的改写，将所有的测试输出进行记录。改写脚本在文章的最后。 改写过程中发现这样一个问题，比如写下面的脚本： 123456789101112#!/bin/bashresult=&quot;&quot;cat abtest_temp.log | while read LINEdo result=`echo $LINE | grep &#x27;Requests per second:&#x27;` if [ &quot;$result&quot; != &quot;&quot; ] then break fidoneecho &quot;result is &quot;$&#123;result&#125;","text":"今天研究apache ab这个测试工具，在网上看到压力测试shell脚本一文介绍了一个封装的bash脚本，用于多次测试返回requests per second的平均值，对脚本进行了简单的改写，将所有的测试输出进行记录。改写脚本在文章的最后。 改写过程中发现这样一个问题，比如写下面的脚本： 123456789101112#!/bin/bashresult=&quot;&quot;cat abtest_temp.log | while read LINEdo result=`echo $LINE | grep &#x27;Requests per second:&#x27;` if [ &quot;$result&quot; != &quot;&quot; ] then break fidoneecho &quot;result is &quot;$&#123;result&#125; 在读取abtest_temp.log文件内容后，result的值仍为空，这是因为bash遇到管道后会创建一个新的进程，于是result是subshell中的局域变量，subshell对变量的修改不会影响原shell中的变量。 subshell可以export父shell中的变量，但export出来的变量只是父shell中变量的一个拷贝，进行修改并不能影响到父shell。但反过来，父shell再次更改此变量时，subshell 再去读时，读到的是新值，而不是原来的值。参考bash man page中的说明：Each command in a pipeline is executed in its own subshell以及Each command in a pipeline is executed as a separate process (i.e., in a subshell). 对于这种情形有一些解决方法，这里给出两种：第一种是将subshell外需要访问的变量输出到临时文件中。第二种是使用命名管道。本质都是进程间通信的实现。 使用临时文件12345678910111213#!/bin/bashresult=&quot;&quot;cat abtest_temp.log | while read LINEdo result=`echo $LINE | grep &#x27;Requests per second:&#x27;` if [ &quot;$result&quot; != &quot;&quot; ] then echo $result &gt; .result_temp break fidoneecho &quot;result is &quot;`cat .result_temp` 使用命名管道1234567891011121314151617#!/bin/bashresult=&quot;&quot;mkfifo pipetem(cat abtest_temp.log | while read LINEdo result=`echo $LINE | grep &#x27;Requests per second:&#x27;` if [ &quot;$result&quot; != &quot;&quot; ] then echo $result &gt; pipetem &amp; break fidone)read result &lt; pipetemrm pipetemecho &quot;result is &quot;$&#123;result&#125; 对apache ab封装的测试脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#!/bin/bashtotal_request=1000concurrency=100times=1cmd_idx=1param_count=$#while [ $cmd_idx -lt $param_count ]do cmd=$1 shift 1 #remove $1 case $cmd in -n) total_request=$1 shift 1;; -c) concurrency=$1 shift 1;; -t) times=$1 shift 1;; *) echo &quot;$cmd, support parameter: -n, -c, -t&quot;;; esac cmd_idx=`expr $cmd_idx + 2`doneurl=$1if [ $url = &#x27;&#x27; ]; then echo &#x27;the test url must be provided...&#x27; exit 2fiecho &quot;Total Request: $total_request, Concurrency: $concurrency, URL: $url, Times: $times&quot;ab_dir=/usr/binab_cmd=&quot;$ab_dir/ab -n $total_request -c $concurrency $url&quot;echo $ab_cmdidx=1rps_sum=0max=-1min=99999999while [ $idx -le $times ]do echo &quot;start loop $idx&quot; $ab_cmd &gt;abtest_temp.log 2&gt;&amp;1 cat abtest_temp.log | while read LINE do result=`echo $LINE | grep &#x27;Requests per second:&#x27;` if [ &quot;$result&quot; != &quot;&quot; ] then echo $result &gt; .result_temp break fi done result=`cat .result_temp` rm .result_temp result=`echo $result | awk -F &#x27; &#x27; &#x27;&#123; print $4 &#125;&#x27; | awk -F &#x27;.&#x27; &#x27;&#123; print $1 &#125;&#x27;` rps_sum=`expr $result + $rps_sum` if [ $result -gt $max ]; then max=$result fi if [ $result -lt $min ]; then min=$result fi idx=`expr $idx + 1`doneecho &quot;avg rps: &quot;`expr $rps_sum / $times`echo &quot;min rps: $min&quot;echo &quot;max rps: $max&quot; 参考文章压力测试shell脚本，实例解析shell子进程（subshell )，小心bash的管道","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[]},{"title":"how to build this blog","slug":"how-to-build-this-blog","date":"2012-12-30T00:00:00.000Z","updated":"2020-12-13T07:04:36.651Z","comments":true,"path":"2012/12/30/how-to-build-this-blog/","link":"","permalink":"http://amyangfei.me/2012/12/30/how-to-build-this-blog/","excerpt":"系统环境Mac OS X 10.8.2，这个博客主要通过jekyll生成静态页面，使用ruby提供的一些扩展插件，比如按月份进行归档等，配置环境的过程如下。 安装ruby，rubygems 1234sudo port rubysudo port install rb-rubygemssudo gem install rubygems-updatesudo gem update --system 安装bundle 1sudo gem install bundler","text":"系统环境Mac OS X 10.8.2，这个博客主要通过jekyll生成静态页面，使用ruby提供的一些扩展插件，比如按月份进行归档等，配置环境的过程如下。 安装ruby，rubygems 1234sudo port rubysudo port install rb-rubygemssudo gem install rubygems-updatesudo gem update --system 安装bundle 1sudo gem install bundler 安装jeklly，rdiscount（支持markdown语法，rdiscount github），liquid（支持jekyll模板渲染，Liquid Extensions）。可以写到一个Gemfile文件中然后 bundle install。 1bundle install Gemfile文件内容 123456source &quot;http://rubygems.org&quot;gem &quot;jekyll&quot;, &quot;0.10.0&quot;gem &quot;jekyll_ext&quot;gem &quot;liquid&quot;, &quot;2.2.2&quot;gem &quot;rdiscount&quot; 关于jekyll的一些扩展插件，可以参考 jekyll plugins 和 rfelix’s jelyll extensions 。 生成静态页面，运行服务器 1bundle exec ejekyll --server","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"jekyll","slug":"jekyll","permalink":"http://amyangfei.me/tags/jekyll/"},{"name":"ruby","slug":"ruby","permalink":"http://amyangfei.me/tags/ruby/"}]},{"title":"随机排序算法简介","slug":"shuffle-algorithm","date":"2012-11-29T00:00:00.000Z","updated":"2020-12-13T07:05:00.262Z","comments":true,"path":"2012/11/29/shuffle-algorithm/","link":"","permalink":"http://amyangfei.me/2012/11/29/shuffle-algorithm/","excerpt":"前几天看了酷壳上的一篇文章如何测试洗牌程序，之后仔细看了Wikipedia对Fisher–Yates shuffle算法的介绍，这里简单的总结一下，基本是翻译Wikipedia。 Fisher and Yates’ original method该算法最初是1938年由Ronald A. Fisher和Frank Yates在《Statistical tables for biological, agricultural and medical research》一书中描述，算法生成1-N个数的随机排列的过程如下： 原始数组中有数字1到N 设原始数组未被标记的数字个数为k，生成一个1到k之间的随机数 取出原始数组未被标记数字中的第k个，将其标记并插入到新的排列数组尾端。 重复过程2直到原始数组中没有未被标记的数字 过程3中生成的新数组就是一个对原始数组的随机排列 该算法可以理解为已知有n个元素，先从n个元素中任选一个，放入新空间的第一个位置，然后再从剩下的n-1个元素中任选一个，放入第二个位置，依此类推。算法在过程3查找未被标记的第k个数字有很多重复操作，导致算法效率并不高，总的时间复杂度为O(N^2 )，空间复杂度为O(N)。算法的python实现如下所示：","text":"前几天看了酷壳上的一篇文章如何测试洗牌程序，之后仔细看了Wikipedia对Fisher–Yates shuffle算法的介绍，这里简单的总结一下，基本是翻译Wikipedia。 Fisher and Yates’ original method该算法最初是1938年由Ronald A. Fisher和Frank Yates在《Statistical tables for biological, agricultural and medical research》一书中描述，算法生成1-N个数的随机排列的过程如下： 原始数组中有数字1到N 设原始数组未被标记的数字个数为k，生成一个1到k之间的随机数 取出原始数组未被标记数字中的第k个，将其标记并插入到新的排列数组尾端。 重复过程2直到原始数组中没有未被标记的数字 过程3中生成的新数组就是一个对原始数组的随机排列 该算法可以理解为已知有n个元素，先从n个元素中任选一个，放入新空间的第一个位置，然后再从剩下的n-1个元素中任选一个，放入第二个位置，依此类推。算法在过程3查找未被标记的第k个数字有很多重复操作，导致算法效率并不高，总的时间复杂度为O(N^2 )，空间复杂度为O(N)。算法的python实现如下所示： 12345678910111213from random import randomdef FisherYateOldShullfe(items): ret = [None] * len(items) for i in reversed(range(0, len(items))): j = int(random() * (i+1)) ret[i] = items[j] del items[j] return retif __name__ == &#x27;__main__&#x27;: srclist = [n for n in range(10)] print FisherYateOldShullfe(srclist) Modern version of the Fisher–Yates shuffle改进版的Fisher–Yates shuffle算法是1964年Richard Durstenfeld在 Communications of the ACM volume 7, issue 7中首次提出，相比于原始Fisher-Yates shuffle最大的改进是不需要在步骤3中重复的数未被标记的数字，该算法不再将标记过的数字移动到一个新数组的尾端，而是将随机数选出的数字与排在最后位置的未标记数字进行交换。算法在python下的实现如下所示： 1234567891011from random import randomdef FisherYatesShuffle(items): for i in reversed(range(1, len(items))): j = int(random() * (i+1)) items[i], items[j] = items[j], items[i]if __name__ == &#x27;__main__&#x27;: srclist = [n for n in range(10)] FisherYatesShuffle(srclist) print srclist 该算法同样可以理解成为这样的过程：从1到n个数字中依次随机抽取一个数字，并放到一个新序列的尾端（该算法通过互换数字实现），逐渐形成一个新的序列。计算一下概率：如果某个元素被放入第i（1≤i≤n）个位置，就必须是在前 i-1 次选取中都没有选到它，并且第 i 次恰好选中它。其概率为： 算法中只有一个从1到N-1的循环，循环内操作为常数步，因而算法总的时间复杂度为O(N)，空间复杂度为O(1)。 Inside-out algorithmFisher-Yates shuffle是一种在原地交换的生成过程，即给定一个序列，算法在这个序列本身的存储空间进行操作。与这种in-place的方式不同，inside-out针对给定序列，会生成该序列随机排列的一个副本。这种方法有利于对长度较大的序列进行随机排列。 Inside-out算法的基本思想是从前向后扫描，依次增加i，每一步操作中将新数组位置i的数字更新为原始数组位置i的数字，然后在新数组中将位置i和随机选出的位置j（0≤j≤i）交换数字。算法亦可以理解为现将原始数组完全复制到新数组，然后新数组位置i(i from 1 to n-1)依次和随机位置j交换数字。算法的python实现如下： 1234567891011121314from random import randomdef insideout(source): ret = [None] * len(source) ret[0] = source[0] for i in range(1, len(source)): j = int(random() * (i+1)) ret[i] = ret[j] ret[j] = source[i] return retif __name__ == &#x27;__main__&#x27;: srclist = [n for n in range(10)] print insideout(srclist) 对于这个算法，我们分析可以出现多少种不同的排列数，从$i=1$开始，每一次交换都可以衍生出$(i+1)$倍的排列数，因而总的排列方案数如下图。在随机函数完全随机的情况下每一种排列都是等概率出现的，因而这种算法得到的是一个随机排序。它的时间复杂度和空间复杂度都是O(N)。 该算法有一个优点就是可以通过不断读取原始数组的下一个元素同时使新的排列数组长度加一，进而生成一个随机排列，即可以对长度未知的序列进行随机排列。实现的伪代码如下： 1234567while source.moreDataAvailable j &lt;- random integer with 0 &lt;= j &lt;= a.length if j = a.length a.append(source.next) else a.append(a[j]) a[j] &lt;- source.next 另一种想法对n个元素的随机排序对应于这n个元素全排列中的一种，所以有这样一种方法求随机排序：求n个元素的随机排列，给定一个随机数k（1≤k≤n!），取出n!个全排列中的第k个即是一种随机排序。于是需要解决2个问题：一是在一个足够大的范围内求随机数；另外是实现一种是在n!个全排列中求第k个全排列的方法。第一个问题很古老，有人说随机数的最大范围决定于随即种子的大小，我有一种想法是对分段求随机数，比如需要求最大范围为N的随机数，那么可以对N进行M进制分解，分别求M进制下的每一位的随机数，最后合成一个大的随机数；而第二个问题就比较容易了，有很多全排列生成算法，通过“原排列”-&gt;“原中介数”-&gt;“新中介数”-&gt;“新排列”的过程，可以很方便的求出第k个全排列。 参考文章洗牌程序，等概率随机排列数组（洗牌算法）","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[]},{"title":"A brief introduction to BIDE algorithm","slug":"bide-algorithm","date":"2012-03-26T00:00:00.000Z","updated":"2020-12-13T07:04:28.455Z","comments":true,"path":"2012/03/26/bide-algorithm/","link":"","permalink":"http://amyangfei.me/2012/03/26/bide-algorithm/","excerpt":"背景最近在做的数据库活动检测引擎，要实现一个异常行为检测的模块。简单的说异常行为检测就是先确定描述用户访问数据库的行为模型，然后定义一系列描述正常访问数据库的行为，检测过程中一旦用户行为与正常行为轮廓不符，则被认定为攻击行为。在实际系统中，正常行为轮廓会组成正常行为知识库，这个知识库会很大，所以正常行为轮廓不能简单的人工定义，于是一个自动挖掘正常行为知识库的需求被提了出来。 在系统实现过程中，会将用户对数据库产生影响的原子性操作，比如一次select，一次update抽象成为一个item，用户访问数据库的一系列行为可以抽象成为一个item的序列，由此引出了通过频繁序列挖掘来获得正常行为知识库的想法。 频繁序列挖掘","text":"背景最近在做的数据库活动检测引擎，要实现一个异常行为检测的模块。简单的说异常行为检测就是先确定描述用户访问数据库的行为模型，然后定义一系列描述正常访问数据库的行为，检测过程中一旦用户行为与正常行为轮廓不符，则被认定为攻击行为。在实际系统中，正常行为轮廓会组成正常行为知识库，这个知识库会很大，所以正常行为轮廓不能简单的人工定义，于是一个自动挖掘正常行为知识库的需求被提了出来。 在系统实现过程中，会将用户对数据库产生影响的原子性操作，比如一次select，一次update抽象成为一个item，用户访问数据库的一系列行为可以抽象成为一个item的序列，由此引出了通过频繁序列挖掘来获得正常行为知识库的想法。 频繁序列挖掘 在介绍频繁序列挖掘之前先简单说一下关联规则挖掘，关联规则挖掘用于从大量数据中挖掘出有价值的数据项之间的相关关系。关联规则解决的常见问题如：“如果一个消费者购买了产品A，那么他有多大机会购买产品B?”以及“如果他购买了产品C和D，那么他还将购买什么产品？”。（引自维基百科），关联规则挖掘领域最著名的故事就是啤酒和尿布的故事了，比较经典的算法有Apriori，Eclat，FP-Growth等。 频繁序列挖掘（Sequence Mining）与关联规则挖掘类似，不同的是序列挖掘结果的各个项目之间是有序的。早期比较著名的序列挖掘算法有GSP，SPADE，PrefixSpan，BIDE算法是2004年被提出，它极大地提高了序列挖掘的性能。这里有一篇对各种序列挖掘算法详分析对比的论文：Frequent pattern mining: current status and future directions BIDE算法的实现描述提出BIDE算法的论文：BIDE:efficient mining of frequent closed sequences，下面按照论文中的一个例子对算法进行简单的介绍。 序列集合 Sequence identifier Sequence content 1 CAABC 2 ABCB 3 CABC 4 ABBCA frequent sequences A:4, AA:2, AB:4, ABB:2, ABC:4, AC:4, B:4, BB:2,BC:4, C:4, CA:3, CAB:2, CABC:2, CAC:2, CB:3, CBC:2,CC:2 frequent closed sequences AA:2, ABB:2, ABC:4, CA:3, CABC:2,CB:3 搜索树： 一些背景知识： 支持度：有绝对支持度和相对支持度两种，相对支持度是绝对支持度除以总序列数。上表中A，B，C为独立的项（item），不同item的组合形成一个序列，如果序列a中的每个item都在序列b中出现，并且是保序的，那么a是b的子序列。序列数据库（SDB）包含一些序列，一个序列a的所有父序列的总数目即为序列a在SDB中的绝对支持度，简单的说一个序列的支持度就是序列在SDB中出现的次数（在同一条序列中多次出现只记一次）。比如在上表中，A在CAABC中出现2次，计算支持度时只记1。 最小支持度：挖掘时候需要定义的常量，支持度大于最小支持度的序列被认为是频繁序列。 闭序列：序列可以进行扩展，比如上表序列1中包括序列CAB，可以扩展成为CABC。如果一个序列的支持度和它的一个扩展序列的支持度相等，那么这个序列就不是闭序列。反之，如果一个序列找不到一个扩展，使得扩展序列支持度与原序列支持度相等，那么这个序列就是闭序列。 算法要点 1. 利用BackSpan search space Pruning减少不必要的搜索过程。比如在搜索树中，B开头的序列都会被AB开头的序列包含，所以以B开头的子树可以不用搜索。 2. 利用forward-extension 和 backward extension 检测一个序列是否为闭序列，如果不是闭序列则不会包含在结果集中。 总结BIDE算法的高效体现在两个方面：一是挖掘过程中不需要产生临时的频繁项集，这极大的节约了存储空间；另一方面由于算法只挖掘频繁闭序列，挖掘过程中会减少很多非闭序列的挖掘过程。这两点加起来使得BIDE算法在性能上有了很大的提升。","categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://amyangfei.me/tags/algorithm/"},{"name":"frequent sequence mining","slug":"frequent-sequence-mining","permalink":"http://amyangfei.me/tags/frequent-sequence-mining/"}]},{"title":"Hello world","slug":"hello-world","date":"2012-03-20T00:00:00.000Z","updated":"2020-12-13T05:02:25.353Z","comments":true,"path":"2012/03/20/hello-world/","link":"","permalink":"http://amyangfei.me/2012/03/20/hello-world/","excerpt":"","text":"似乎应该在这里说些什么。。。","categories":[{"name":"others","slug":"others","permalink":"http://amyangfei.me/categories/others/"}],"tags":[]}],"categories":[{"name":"program","slug":"program","permalink":"http://amyangfei.me/categories/program/"},{"name":"others","slug":"others","permalink":"http://amyangfei.me/categories/others/"}],"tags":[{"name":"streaming system","slug":"streaming-system","permalink":"http://amyangfei.me/tags/streaming-system/"},{"name":"change data capture","slug":"change-data-capture","permalink":"http://amyangfei.me/tags/change-data-capture/"},{"name":"book reading notes","slug":"book-reading-notes","permalink":"http://amyangfei.me/tags/book-reading-notes/"},{"name":"golang","slug":"golang","permalink":"http://amyangfei.me/tags/golang/"},{"name":"etcd","slug":"etcd","permalink":"http://amyangfei.me/tags/etcd/"},{"name":"source code reading","slug":"source-code-reading","permalink":"http://amyangfei.me/tags/source-code-reading/"},{"name":"redis","slug":"redis","permalink":"http://amyangfei.me/tags/redis/"},{"name":"redis module","slug":"redis-module","permalink":"http://amyangfei.me/tags/redis-module/"},{"name":"python","slug":"python","permalink":"http://amyangfei.me/tags/python/"},{"name":"job queue","slug":"job-queue","permalink":"http://amyangfei.me/tags/job-queue/"},{"name":"ssh","slug":"ssh","permalink":"http://amyangfei.me/tags/ssh/"},{"name":"proxy","slug":"proxy","permalink":"http://amyangfei.me/tags/proxy/"},{"name":"task scheduler","slug":"task-scheduler","permalink":"http://amyangfei.me/tags/task-scheduler/"},{"name":"virtualization","slug":"virtualization","permalink":"http://amyangfei.me/tags/virtualization/"},{"name":"tornado","slug":"tornado","permalink":"http://amyangfei.me/tags/tornado/"},{"name":"asynchronous programming","slug":"asynchronous-programming","permalink":"http://amyangfei.me/tags/asynchronous-programming/"},{"name":"c++","slug":"c","permalink":"http://amyangfei.me/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://amyangfei.me/tags/stl/"},{"name":"llvm","slug":"llvm","permalink":"http://amyangfei.me/tags/llvm/"},{"name":"web framework","slug":"web-framework","permalink":"http://amyangfei.me/tags/web-framework/"},{"name":"android","slug":"android","permalink":"http://amyangfei.me/tags/android/"},{"name":"asynchronous I/O","slug":"asynchronous-I-O","permalink":"http://amyangfei.me/tags/asynchronous-I-O/"},{"name":"web server framework","slug":"web-server-framework","permalink":"http://amyangfei.me/tags/web-server-framework/"},{"name":"asynchronous","slug":"asynchronous","permalink":"http://amyangfei.me/tags/asynchronous/"},{"name":"jekyll","slug":"jekyll","permalink":"http://amyangfei.me/tags/jekyll/"},{"name":"ruby","slug":"ruby","permalink":"http://amyangfei.me/tags/ruby/"},{"name":"algorithm","slug":"algorithm","permalink":"http://amyangfei.me/tags/algorithm/"},{"name":"frequent sequence mining","slug":"frequent-sequence-mining","permalink":"http://amyangfei.me/tags/frequent-sequence-mining/"}]}